{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install kafka-python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ingesting data in Apache Kafka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kafka import KafkaProducer\n",
    "from time import sleep\n",
    "\n",
    "import pandas as pd\n",
    "import random\n",
    "import decimal\n",
    "import json\n",
    "import uuid\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KAFKA_HOST = \"192.168.49.2:30323\"\n",
    "\n",
    "producer = KafkaProducer(bootstrap_servers=KAFKA_HOST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KAFKA_TOPIC = \"demo-users\"\n",
    "\n",
    "for i in range(10):\n",
    "\n",
    "  # create a dictionary of name and email random data to send to Kafka\n",
    "  message = {\"id\": str(uuid.uuid4()), \"name\": \"name_\" + str(i), \"email\": \"email_\" + str(i) + \"@example.com\"}\n",
    "\n",
    "  producer.send(KAFKA_TOPIC, json.dumps(message).encode(\"utf-8\"))\n",
    "  producer.flush()\n",
    "\n",
    "  print(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KAFKA_TOPIC = \"demo-logs\"\n",
    "\n",
    "for i in range(10):\n",
    "\n",
    "  # create a dictionary of cpu and memory random data to send to Kafka\n",
    "  message = {\"id\": i, \"cpu\": round(random.uniform(0, 100), 2), \"memory\": round(random.uniform(0, 100), 2)}\n",
    "  producer.send(KAFKA_TOPIC, json.dumps(message).encode(\"utf-8\"))\n",
    "  producer.flush()\n",
    "\n",
    "  print(message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading data from Apache Kafka with Apache Spark Structured Streaming\n",
    "\n",
    "https://spark.apache.org/docs/latest/structured-streaming-kafka-integration.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import (\n",
    "    StructType,\n",
    "    StructField,\n",
    "    FloatType,\n",
    "    StringType,\n",
    "    LongType,\n",
    "    IntegerType,\n",
    "    DecimalType,\n",
    ")\n",
    "from pyspark.sql.types import (\n",
    "    StructType,\n",
    "    StructField,\n",
    "    FloatType,\n",
    "    StringType,\n",
    "    LongType,\n",
    "    IntegerType,\n",
    "    DoubleType,\n",
    ")\n",
    "from pyspark.sql.functions import (\n",
    "    split,\n",
    "    regexp_replace,\n",
    "    current_date,\n",
    "    unix_timestamp,\n",
    "    lit,\n",
    "    current_timestamp,\n",
    ")\n",
    "\n",
    "from pyspark.sql.functions import col, from_json, struct, to_json\n",
    "from pyspark.sql.functions import pandas_udf, PandasUDFType\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import json\n",
    "import time\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\n",
    "    \"PYSPARK_SUBMIT_ARGS\"\n",
    "] = \"--packages org.apache.spark:spark-streaming-kafka-0-10_2.12:3.2.0,org.apache.spark:spark-sql-kafka-0-10_2.12:3.2.0,org.postgresql:postgresql:42.1.1 pyspark-shell\"\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"Spark Structured Streaming Application\")\n",
    "    .master(\"local[*]\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# readStream from Kafka\n",
    "\n",
    "**Required configurations:**\n",
    "\n",
    "- kafka.bootstrap.servers: A comma-separated list of host:port\n",
    "- subscribe: A comma-separated list of topics\n",
    "- \n",
    "\n",
    "**Optional configurations:**\n",
    "\n",
    "- startingTimestamp\n",
    "- startingOffsetsByTimestamp\n",
    "- startingOffsets:\n",
    "  - \"latest\" for streaming\n",
    "  - \"earliest\" for batch\n",
    "  - {\"topicA\":{\"0\":23,\"1\":-1}} for specifying a starting offset for each TopicPartition\n",
    "  - \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subscribe to demo topic\n",
    "df = spark \\\n",
    "  .readStream \\\n",
    "  .format(\"kafka\") \\\n",
    "  .option(\"kafka.bootstrap.servers\", KAFKA_HOST) \\\n",
    "  .option(\"subscribe\", \"demo, demo-logs\") \\\n",
    "  .option(\"startingOffsets\", \"earliest\") \\\n",
    "  .load()\n",
    "\n",
    "# df = df.selectExpr(\"CAST(value AS STRING)\")\n",
    "\n",
    "query = df \\\n",
    "    .where(col(\"topic\") == \"demo\") \\\n",
    "    .writeStream \\\n",
    "    .format(\"console\") \\\n",
    "    .start()\n",
    "\n",
    "time.sleep(10) # sleep 10 seconds\n",
    "\n",
    "query.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subscribe to demo users and logs topic\n",
    "df = spark \\\n",
    "  .readStream \\\n",
    "  .format(\"kafka\") \\\n",
    "  .option(\"kafka.bootstrap.servers\", KAFKA_HOST) \\\n",
    "  .option(\"subscribe\", \"demo-users, demo-logs\") \\\n",
    "  .option(\"startingOffsets\", \"earliest\") \\\n",
    "  .load()\n",
    "\n",
    "df = df.selectExpr(\"CAST(value AS STRING)\")\n",
    "\n",
    "query = df \\\n",
    "    .writeStream \\\n",
    "    .format(\"console\") \\\n",
    "    .start()\n",
    "\n",
    "time.sleep(10) # sleep 10 seconds\n",
    "\n",
    "query.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subscribe to demo users and logs topic\n",
    "df = spark \\\n",
    "  .readStream \\\n",
    "  .format(\"kafka\") \\\n",
    "  .option(\"kafka.bootstrap.servers\", KAFKA_HOST) \\\n",
    "  .option(\"subscribe\", \"demo-users, demo-logs\") \\\n",
    "  .option(\"startingOffsets\", \"earliest\") \\\n",
    "  .load()\n",
    "\n",
    "df = df.selectExpr(\"CAST(value AS STRING)\")\n",
    "\n",
    "# query = df \\\n",
    "#     .writeStream \\\n",
    "#     .format(\"console\") \\\n",
    "#     .start()\n",
    "\n",
    "# time.sleep(10) # sleep 10 seconds\n",
    "\n",
    "# query.stop()\n",
    "\n",
    "# write the datastream to json file\n",
    "query = df \\\n",
    "    .writeStream \\\n",
    "    .format(\"json\") \\\n",
    "    .option(\"path\", \"demo-users-and-logs\") \\\n",
    "    .option(\"checkpointLocation\", \"checkpointLocation\") \\\n",
    "    .start()\n",
    "\n",
    "query.awaitTermination()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat demo-users-and-logs/part-00000-*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subscribe to 1 topic, with headers\n",
    "# TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subscribe to 1 topic defaults to the earliest and latest offsets\n",
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## startingOffsets\n",
    "\n",
    "\"latest\" for streaming, \"earliest\" for batch\n",
    "\n",
    "The start point when a query is started, either \"earliest\" which is from the earliest offsets, \"latest\" which is just from the latest offsets, or a json string specifying a starting offset for each TopicPartition. In the json, -2 as an offset can be used to refer to earliest, -1 to latest. Note: For batch queries, latest (either implicitly or by using -1 in json) is not allowed. For streaming queries, this only applies when a new query is started, and that resuming will always pick up from where the query left off. Newly discovered partitions during a query will start at earliest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subscribe to multiple topics, specifying explicit Kafka offsets\n",
    "\n",
    "df = spark \\\n",
    "  .read \\\n",
    "  .format(\"kafka\") \\\n",
    "  .option(\"kafka.bootstrap.servers\", \"host1:port1,host2:port2\") \\\n",
    "  .option(\"subscribe\", KAFKA_TOPIC) \\\n",
    "  .option(\"startingOffsets\", \"\"\"{\"topic1\":{\"0\":23,\"1\":-2}\"\"\") \\\n",
    "  .option(\"endingOffsets\", \"\"\"{\"topic1\":{\"0\":50,\"1\":-1}\"\"\") \\\n",
    "  .load()\n",
    "\n",
    "df = df.selectExpr(\"CAST(value AS STRING)\")\n",
    "\n",
    "query = df \\\n",
    "    .writeStream \\\n",
    "    .format(\"console\") \\\n",
    "    .start()\n",
    "\n",
    "time.sleep(10) # sleep 10 seconds\n",
    "\n",
    "query.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = values.writeStream \\\n",
    "    .format(\"json\") \\\n",
    "    .outputMode(\"append\")\n",
    "    .start(\"./topic.json\")\n",
    "    \n",
    "import time\n",
    "\n",
    "time.sleep(10) # sleep 10 seconds\n",
    "\n",
    "query.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get current timestamp\n",
    "df = df.withColumn(\"timestamp\", current_timestamp())\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "23290619d6f4f5ecea54fd3814d63b8a6a2d2c019f5870989b08cbcfb848aa36"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 ('spark-ml-training-Z1-rjbZ7-py3.9')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
