{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f794456",
   "metadata": {},
   "source": [
    "# Spark Structured Streaming Application\n",
    "\n",
    "This notebook contains ... TODO\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6188f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install pre-requisites\n",
    "!pip install ipython-sql psycopg2-binary pyspark==3.2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9fa01d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "%load_ext sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b478262e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import (\n",
    "    StructType,\n",
    "    StructField,\n",
    "    FloatType,\n",
    "    StringType,\n",
    "    LongType,\n",
    "    IntegerType,\n",
    "    DecimalType,\n",
    ")\n",
    "from pyspark.sql.types import (\n",
    "    StructType,\n",
    "    StructField,\n",
    "    FloatType,\n",
    "    StringType,\n",
    "    LongType,\n",
    "    IntegerType,\n",
    "    DoubleType,\n",
    ")\n",
    "from pyspark.sql.functions import (\n",
    "    split,\n",
    "    regexp_replace,\n",
    "    current_date,\n",
    "    unix_timestamp,\n",
    "    lit,\n",
    "    current_timestamp,\n",
    ")\n",
    "\n",
    "from pyspark.sql.functions import col, from_json, struct, to_json\n",
    "from pyspark.sql.functions import pandas_udf, PandasUDFType\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import json\n",
    "import time\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d3ba7b97",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-06T17:04:27.014515Z",
     "iopub.status.busy": "2022-06-06T17:04:27.013980Z",
     "iopub.status.idle": "2022-06-06T17:04:33.367796Z",
     "shell.execute_reply": "2022-06-06T17:04:33.367050Z",
     "shell.execute_reply.started": "2022-06-06T17:04:27.014449Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/06/09 01:48:39 WARN Utils: Your hostname, carloshkayser resolves to a loopback address: 127.0.1.1; using 10.32.45.215 instead (on interface ens160)\n",
      "22/06/09 01:48:39 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/kayser/spark-3.2.1-bin-hadoop3.2/jars/spark-unsafe_2.12-3.2.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/home/kayser/spark-3.2.1-bin-hadoop3.2/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/kayser/.ivy2/cache\n",
      "The jars for the packages stored in: /home/kayser/.ivy2/jars\n",
      "org.apache.spark#spark-streaming-kafka-0-10_2.12 added as a dependency\n",
      "org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency\n",
      "org.postgresql#postgresql added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-c97df09a-1c8a-4da6-874a-b1644037148d;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.spark#spark-streaming-kafka-0-10_2.12;3.2.0 in central\n",
      "\tfound org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.2.0 in central\n",
      "\tfound org.apache.kafka#kafka-clients;2.8.0 in central\n",
      "\tfound org.lz4#lz4-java;1.7.1 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.8.4 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.30 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-runtime;3.3.1 in central\n",
      "\tfound org.spark-project.spark#unused;1.0.0 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-api;3.3.1 in central\n",
      "\tfound org.apache.htrace#htrace-core4;4.1.0-incubating in central\n",
      "\tfound commons-logging#commons-logging;1.1.3 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.0 in central\n",
      "\tfound org.apache.spark#spark-sql-kafka-0-10_2.12;3.2.0 in central\n",
      "\tfound org.apache.commons#commons-pool2;2.6.2 in central\n",
      "\tfound org.postgresql#postgresql;42.1.1 in central\n",
      ":: resolution report :: resolve 524ms :: artifacts dl 14ms\n",
      "\t:: modules in use:\n",
      "\tcom.google.code.findbugs#jsr305;3.0.0 from central in [default]\n",
      "\tcommons-logging#commons-logging;1.1.3 from central in [default]\n",
      "\torg.apache.commons#commons-pool2;2.6.2 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-api;3.3.1 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-runtime;3.3.1 from central in [default]\n",
      "\torg.apache.htrace#htrace-core4;4.1.0-incubating from central in [default]\n",
      "\torg.apache.kafka#kafka-clients;2.8.0 from central in [default]\n",
      "\torg.apache.spark#spark-sql-kafka-0-10_2.12;3.2.0 from central in [default]\n",
      "\torg.apache.spark#spark-streaming-kafka-0-10_2.12;3.2.0 from central in [default]\n",
      "\torg.apache.spark#spark-token-provider-kafka-0-10_2.12;3.2.0 from central in [default]\n",
      "\torg.lz4#lz4-java;1.7.1 from central in [default]\n",
      "\torg.postgresql#postgresql;42.1.1 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.30 from central in [default]\n",
      "\torg.spark-project.spark#unused;1.0.0 from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.8.4 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   15  |   0   |   0   |   0   ||   15  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-c97df09a-1c8a-4da6-874a-b1644037148d\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 15 already retrieved (0kB/11ms)\n",
      "22/06/09 01:48:40 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.32.45.215:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Spark Structured Streaming Application</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f8a2fae41f0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ[\n",
    "    \"PYSPARK_SUBMIT_ARGS\"\n",
    "] = \"--packages org.apache.spark:spark-streaming-kafka-0-10_2.12:3.2.0,org.apache.spark:spark-sql-kafka-0-10_2.12:3.2.0,org.postgresql:postgresql:42.1.1 pyspark-shell\"\n",
    "\n",
    "\n",
    "APP_NAME = os.getenv(\"APP_NAME\", \"spark-streaming-app\")\n",
    "MASTER = os.getenv(\"MASTER\", \"local[*]\")\n",
    "\n",
    "# 192.168.49.2 == minikube ip\n",
    "# 30323 == NodePort\n",
    "\n",
    "KAFKA_HOST = \"192.168.49.2:30323\"  # \"kafka-cluster-kafka-bootstrap:9092\"\n",
    "\n",
    "# MASTER = \"spark://carloshkayser:7077\"\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"Spark Structured Streaming Application\")\n",
    "    .master(MASTER)\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "spark.sparkContext.setLogLevel('ERROR')\n",
    "\n",
    "spark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8286dabe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-06T17:04:33.369282Z",
     "iopub.status.busy": "2022-06-06T17:04:33.368885Z",
     "iopub.status.idle": "2022-06-06T17:04:34.892328Z",
     "shell.execute_reply": "2022-06-06T17:04:34.891611Z",
     "shell.execute_reply.started": "2022-06-06T17:04:33.369251Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_raw = (\n",
    "    spark.readStream.format(\"kafka\")\n",
    "    .option(\"kafka.bootstrap.servers\", KAFKA_HOST)\n",
    "    .option(\"subscribe\", \"to_predict\")\n",
    "    .option(\"startingOffsets\", \"latest\")\n",
    "    .load()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a6008327",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-06T17:04:34.893745Z",
     "iopub.status.busy": "2022-06-06T17:04:34.893368Z",
     "iopub.status.idle": "2022-06-06T17:04:34.918896Z",
     "shell.execute_reply": "2022-06-06T17:04:34.918246Z",
     "shell.execute_reply.started": "2022-06-06T17:04:34.893719Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: binary (nullable = true)\n",
      " |-- value: binary (nullable = true)\n",
      " |-- topic: string (nullable = true)\n",
      " |-- partition: integer (nullable = true)\n",
      " |-- offset: long (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- timestampType: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_raw.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d9d2fb02-a207-410b-9575-2f4077bab353",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-06T17:04:34.921377Z",
     "iopub.status.busy": "2022-06-06T17:04:34.920993Z",
     "iopub.status.idle": "2022-06-06T17:04:35.438137Z",
     "shell.execute_reply": "2022-06-06T17:04:35.436614Z",
     "shell.execute_reply.started": "2022-06-06T17:04:34.921352Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# {\n",
    "#   \"id\": 1.0079274744188029e+19,\n",
    "#   \"hour\": 14103100,\n",
    "#   \"C1\": 1005,\n",
    "#   \"banner_pos\": 0,\n",
    "#   \"site_id\": \"85f751fd\",\n",
    "#   \"site_domain\": \"c4e18dd6\",\n",
    "#   \"site_category\": \"50e219e0\",\n",
    "#   \"app_id\": \"febd1138\",\n",
    "#   \"app_domain\": \"82e27996\",\n",
    "#   \"app_category\": \"0f2161f8\",\n",
    "#   \"device_id\": \"a99f214a\",\n",
    "#   \"device_ip\": \"b72692c8\",\n",
    "#   \"device_model\": \"99e427c9\",\n",
    "#   \"device_type\": 1,\n",
    "#   \"device_conn_type\": 0,\n",
    "#   \"C14\": 21611,\n",
    "#   \"C15\": 320,\n",
    "#   \"C16\": 50,\n",
    "#   \"C17\": 2480,\n",
    "#   \"C18\": 3,\n",
    "#   \"C19\": 299,\n",
    "#   \"C20\": 100111,\n",
    "#   \"C21\": 61\n",
    "# }\n",
    "\n",
    "schema = StructType(\n",
    "    [\n",
    "        StructField(\"id\", DecimalType(38, 0), True),\n",
    "        StructField(\"hour\", IntegerType(), True),\n",
    "        StructField(\"C1\", IntegerType(), True),\n",
    "        StructField(\"banner_pos\", IntegerType(), True),\n",
    "        StructField(\"site_id\", StringType(), True),\n",
    "        StructField(\"site_domain\", StringType(), True),\n",
    "        StructField(\"site_category\", StringType(), True),\n",
    "        StructField(\"app_id\", StringType(), True),\n",
    "        StructField(\"app_domain\", StringType(), True),\n",
    "        StructField(\"app_category\", StringType(), True),\n",
    "        StructField(\"device_id\", StringType(), True),\n",
    "        StructField(\"device_ip\", StringType(), True),\n",
    "        StructField(\"device_model\", StringType(), True),\n",
    "        StructField(\"device_type\", IntegerType(), True),\n",
    "        StructField(\"device_conn_type\", IntegerType(), True),\n",
    "        StructField(\"C14\", IntegerType(), True),\n",
    "        StructField(\"C15\", IntegerType(), True),\n",
    "        StructField(\"C16\", IntegerType(), True),\n",
    "        StructField(\"C17\", IntegerType(), True),\n",
    "        StructField(\"C18\", IntegerType(), True),\n",
    "        StructField(\"C19\", IntegerType(), True),\n",
    "        StructField(\"C20\", IntegerType(), True),\n",
    "        StructField(\"C21\", IntegerType(), True),\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b073d71f-22d7-4634-ac56-dd82968f3b3a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-06T17:04:35.440829Z",
     "iopub.status.busy": "2022-06-06T17:04:35.440219Z",
     "iopub.status.idle": "2022-06-06T17:04:35.831027Z",
     "shell.execute_reply": "2022-06-06T17:04:35.830348Z",
     "shell.execute_reply.started": "2022-06-06T17:04:35.440795Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: decimal(38,0) (nullable = true)\n",
      " |-- hour: integer (nullable = true)\n",
      " |-- C1: integer (nullable = true)\n",
      " |-- banner_pos: integer (nullable = true)\n",
      " |-- site_id: string (nullable = true)\n",
      " |-- site_domain: string (nullable = true)\n",
      " |-- site_category: string (nullable = true)\n",
      " |-- app_id: string (nullable = true)\n",
      " |-- app_domain: string (nullable = true)\n",
      " |-- app_category: string (nullable = true)\n",
      " |-- device_id: string (nullable = true)\n",
      " |-- device_ip: string (nullable = true)\n",
      " |-- device_model: string (nullable = true)\n",
      " |-- device_type: integer (nullable = true)\n",
      " |-- device_conn_type: integer (nullable = true)\n",
      " |-- C14: integer (nullable = true)\n",
      " |-- C15: integer (nullable = true)\n",
      " |-- C16: integer (nullable = true)\n",
      " |-- C17: integer (nullable = true)\n",
      " |-- C18: integer (nullable = true)\n",
      " |-- C19: integer (nullable = true)\n",
      " |-- C20: integer (nullable = true)\n",
      " |-- C21: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = (\n",
    "    df_raw.selectExpr(\"CAST(value AS STRING)\")\n",
    "    .select(from_json(\"value\", schema).alias(\"data\"))\n",
    "    .select(\"data.*\")\n",
    ")\n",
    "\n",
    "df.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "60393740-2f87-4fef-b611-40add528fa27",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-06T17:04:35.832359Z",
     "iopub.status.busy": "2022-06-06T17:04:35.832053Z",
     "iopub.status.idle": "2022-06-06T17:04:43.307993Z",
     "shell.execute_reply": "2022-06-06T17:04:43.307302Z",
     "shell.execute_reply.started": "2022-06-06T17:04:35.832335Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: decimal(38,0) (nullable = true)\n",
      " |-- hour: integer (nullable = true)\n",
      " |-- C1: integer (nullable = true)\n",
      " |-- banner_pos: integer (nullable = true)\n",
      " |-- site_id: string (nullable = true)\n",
      " |-- site_domain: string (nullable = true)\n",
      " |-- site_category: string (nullable = true)\n",
      " |-- app_id: string (nullable = true)\n",
      " |-- app_domain: string (nullable = true)\n",
      " |-- app_category: string (nullable = true)\n",
      " |-- device_id: string (nullable = true)\n",
      " |-- device_ip: string (nullable = true)\n",
      " |-- device_model: string (nullable = true)\n",
      " |-- device_type: integer (nullable = true)\n",
      " |-- device_conn_type: integer (nullable = true)\n",
      " |-- C14: integer (nullable = true)\n",
      " |-- C15: integer (nullable = true)\n",
      " |-- C16: integer (nullable = true)\n",
      " |-- C17: integer (nullable = true)\n",
      " |-- C18: integer (nullable = true)\n",
      " |-- C19: integer (nullable = true)\n",
      " |-- C20: integer (nullable = true)\n",
      " |-- C21: integer (nullable = true)\n",
      " |-- site_idIndex: double (nullable = false)\n",
      " |-- site_domainIndex: double (nullable = false)\n",
      " |-- site_categoryIndex: double (nullable = false)\n",
      " |-- app_idIndex: double (nullable = false)\n",
      " |-- app_domainIndex: double (nullable = false)\n",
      " |-- app_categoryIndex: double (nullable = false)\n",
      " |-- device_idIndex: double (nullable = false)\n",
      " |-- device_ipIndex: double (nullable = false)\n",
      " |-- device_modelIndex: double (nullable = false)\n",
      " |-- features_scaled1: vector (nullable = true)\n",
      " |-- features_scaled: vector (nullable = true)\n",
      " |-- site_id_ohe: vector (nullable = true)\n",
      " |-- site_domain_ohe: vector (nullable = true)\n",
      " |-- site_category_ohe: vector (nullable = true)\n",
      " |-- app_id_ohe: vector (nullable = true)\n",
      " |-- app_domain_ohe: vector (nullable = true)\n",
      " |-- app_category_ohe: vector (nullable = true)\n",
      " |-- device_id_ohe: vector (nullable = true)\n",
      " |-- device_ip_ohe: vector (nullable = true)\n",
      " |-- device_model_ohe: vector (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      " |-- rawPrediction: vector (nullable = true)\n",
      " |-- probability: vector (nullable = true)\n",
      " |-- prediction: double (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import PipelineModel\n",
    "\n",
    "# Read the model from disk\n",
    "pipelineModel = PipelineModel.load(\"model/spark-logistic-regression-model\")\n",
    "\n",
    "# Apply machine learning pipeline to the data\n",
    "results = pipelineModel.transform(df)\n",
    "\n",
    "results.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2525f651-7675-4f47-a556-afd0208e0ae8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-06T17:04:46.513075Z",
     "iopub.status.busy": "2022-06-06T17:04:46.512538Z",
     "iopub.status.idle": "2022-06-06T17:04:46.607462Z",
     "shell.execute_reply": "2022-06-06T17:04:46.606690Z",
     "shell.execute_reply.started": "2022-06-06T17:04:46.513052Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "results = results.withColumn(\"processed_at\", current_timestamp())\n",
    "\n",
    "results = (\n",
    "    results.withColumn(\"probability\", results[\"probability\"].cast(\"String\"))\n",
    "    .withColumn(\n",
    "        \"probabilityre\",\n",
    "        split(regexp_replace(\"probability\", \"^\\[|\\]\", \"\"), \",\")[1].cast(DoubleType()),\n",
    "    )\n",
    "    .select(\"id\", \"probabilityre\", \"processed_at\")\n",
    "    .withColumnRenamed(\"probabilityre\", \"probability\")\n",
    ")\n",
    "\n",
    "results_kafka = results.select(\n",
    "    to_json(struct(\"id\", \"probability\", \"processed_at\")).alias(\"value\")\n",
    ")\n",
    "\n",
    "results_postgres = results.select(\n",
    "    \"id\", \"probability\", \"processed_at\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cfed30ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- value: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results_kafka.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3592aea8-f06c-41be-8de0-4033cf930de1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-06T17:04:46.613942Z",
     "iopub.status.busy": "2022-06-06T17:04:46.613243Z",
     "iopub.status.idle": "2022-06-06T17:04:46.617782Z",
     "shell.execute_reply": "2022-06-06T17:04:46.617108Z",
     "shell.execute_reply.started": "2022-06-06T17:04:46.613917Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: decimal(38,0) (nullable = true)\n",
      " |-- probability: double (nullable = true)\n",
      " |-- processed_at: timestamp (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results_postgres.printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1bb0d0b",
   "metadata": {},
   "source": [
    "### Logging the data stream in the console"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5332f99b-a345-4383-94f5-885391a53c50",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-06T17:04:46.621809Z",
     "iopub.status.busy": "2022-06-06T17:04:46.621243Z",
     "iopub.status.idle": "2022-06-06T17:04:46.625335Z",
     "shell.execute_reply": "2022-06-06T17:04:46.624590Z",
     "shell.execute_reply.started": "2022-06-06T17:04:46.621784Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+---+-----------+------------+\n",
      "| id|probability|processed_at|\n",
      "+---+-----------+------------+\n",
      "+---+-----------+------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 1\n",
      "-------------------------------------------\n",
      "+--------------------+------------------+--------------------+\n",
      "|                  id|       probability|        processed_at|\n",
      "+--------------------+------------------+--------------------+\n",
      "|10731237011332368384|0.6400365553800906|2022-06-09 01:49:...|\n",
      "|10731237011332368384|0.6400365553800906|2022-06-09 01:49:...|\n",
      "|10731273124311236608|0.2563480426642468|2022-06-09 01:49:...|\n",
      "|10731237011332368384|0.6400365553800906|2022-06-09 01:49:...|\n",
      "|10731273124311236608|0.2563480426642468|2022-06-09 01:49:...|\n",
      "|10731273124311236608|0.2563480426642468|2022-06-09 01:49:...|\n",
      "+--------------------+------------------+--------------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 2\n",
      "-------------------------------------------\n",
      "+--------------------+-------------------+--------------------+\n",
      "|                  id|        probability|        processed_at|\n",
      "+--------------------+-------------------+--------------------+\n",
      "|10731542609788790784|0.09518122803820539|2022-06-09 01:49:...|\n",
      "|10731542609788790784|0.09518122803820539|2022-06-09 01:49:...|\n",
      "|10731542609788790784|0.09518122803820539|2022-06-09 01:49:...|\n",
      "+--------------------+-------------------+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/06/09 01:49:08 ERROR WriteToDataSourceV2Exec: Data source write support org.apache.spark.sql.execution.streaming.sources.MicroBatchWrite@c17a26 is aborting.\n",
      "22/06/09 01:49:08 ERROR WriteToDataSourceV2Exec: Data source write support org.apache.spark.sql.execution.streaming.sources.MicroBatchWrite@c17a26 aborted.\n",
      "22/06/09 01:49:08 ERROR Utils: Aborting task                        (1 + 1) / 2]\n",
      "org.apache.spark.TaskKilledException\n",
      "\tat org.apache.spark.TaskContextImpl.killTaskIfInterrupted(TaskContextImpl.scala:216)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:36)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:513)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.$anonfun$run$1(WriteToDataSourceV2Exec.scala:412)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1496)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:457)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:358)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "22/06/09 01:49:08 ERROR DataWritingSparkTask: Aborting commit for partition 0 (task 60, attempt 0, stage 57.0)\n",
      "22/06/09 01:49:08 ERROR DataWritingSparkTask: Aborted commit for partition 0 (task 60, attempt 0, stage 57.0)\n"
     ]
    }
   ],
   "source": [
    "query = results.select(\"id\", \"probability\", \"processed_at\") \\\n",
    "    .writeStream \\\n",
    "    .format(\"console\") \\\n",
    "    .start()\n",
    "\n",
    "time.sleep(10)\n",
    "\n",
    "query.stop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb48ce3",
   "metadata": {},
   "source": [
    "### Inserting data stream transformation results into another Apache Kafka topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e553b9fb-9c09-4340-8451-b0e95f653bc2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-06T17:04:46.791477Z",
     "iopub.status.busy": "2022-06-06T17:04:46.790890Z",
     "iopub.status.idle": "2022-06-06T17:04:46.795093Z",
     "shell.execute_reply": "2022-06-06T17:04:46.794281Z",
     "shell.execute_reply.started": "2022-06-06T17:04:46.791450Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = results_kafka.writeStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", KAFKA_HOST) \\\n",
    "    .option(\"topic\", \"predictions\") \\\n",
    "    .option(\"checkpointLocation\", \"checkpointLocation\") \\\n",
    "    .outputMode(\"Append\") \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "63613986",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022-06-09 01:49:10,221] WARN [Consumer clientId=console-consumer, groupId=console-consumer-74275] Connection to node -1 (localhost/127.0.0.1:9092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient)\n",
      "[2022-06-09 01:49:10,222] WARN [Consumer clientId=console-consumer, groupId=console-consumer-74275] Bootstrap broker localhost:9092 (id: -1 rack: null) disconnected (org.apache.kafka.clients.NetworkClient)\n",
      "[2022-06-09 01:49:10,320] WARN [Consumer clientId=console-consumer, groupId=console-consumer-74275] Connection to node -1 (localhost/127.0.0.1:9092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient)\n",
      "[2022-06-09 01:49:10,321] WARN [Consumer clientId=console-consumer, groupId=console-consumer-74275] Bootstrap broker localhost:9092 (id: -1 rack: null) disconnected (org.apache.kafka.clients.NetworkClient)\n",
      "[2022-06-09 01:49:10,421] WARN [Consumer clientId=console-consumer, groupId=console-consumer-74275] Connection to node -1 (localhost/127.0.0.1:9092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient)\n",
      "[2022-06-09 01:49:10,421] WARN [Consumer clientId=console-consumer, groupId=console-consumer-74275] Bootstrap broker localhost:9092 (id: -1 rack: null) disconnected (org.apache.kafka.clients.NetworkClient)\n",
      "[2022-06-09 01:49:10,623] WARN [Consumer clientId=console-consumer, groupId=console-consumer-74275] Connection to node -1 (localhost/127.0.0.1:9092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient)\n",
      "[2022-06-09 01:49:10,623] WARN [Consumer clientId=console-consumer, groupId=console-consumer-74275] Bootstrap broker localhost:9092 (id: -1 rack: null) disconnected (org.apache.kafka.clients.NetworkClient)\n",
      "[2022-06-09 01:49:11,025] WARN [Consumer clientId=console-consumer, groupId=console-consumer-74275] Connection to node -1 (localhost/127.0.0.1:9092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient)\n",
      "[2022-06-09 01:49:11,026] WARN [Consumer clientId=console-consumer, groupId=console-consumer-74275] Bootstrap broker localhost:9092 (id: -1 rack: null) disconnected (org.apache.kafka.clients.NetworkClient)\n",
      "[2022-06-09 01:49:11,880] WARN [Consumer clientId=console-consumer, groupId=console-consumer-74275] Connection to node -1 (localhost/127.0.0.1:9092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient)\n",
      "[2022-06-09 01:49:11,880] WARN [Consumer clientId=console-consumer, groupId=console-consumer-74275] Bootstrap broker localhost:9092 (id: -1 rack: null) disconnected (org.apache.kafka.clients.NetworkClient)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022-06-09 01:49:12,887] WARN [Consumer clientId=console-consumer, groupId=console-consumer-74275] Connection to node -1 (localhost/127.0.0.1:9092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient)\n",
      "[2022-06-09 01:49:12,887] WARN [Consumer clientId=console-consumer, groupId=console-consumer-74275] Bootstrap broker localhost:9092 (id: -1 rack: null) disconnected (org.apache.kafka.clients.NetworkClient)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 59:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022-06-09 01:49:14,044] WARN [Consumer clientId=console-consumer, groupId=console-consumer-74275] Connection to node -1 (localhost/127.0.0.1:9092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient)\n",
      "[2022-06-09 01:49:14,044] WARN [Consumer clientId=console-consumer, groupId=console-consumer-74275] Bootstrap broker localhost:9092 (id: -1 rack: null) disconnected (org.apache.kafka.clients.NetworkClient)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022-06-09 01:49:15,050] WARN [Consumer clientId=console-consumer, groupId=console-consumer-74275] Connection to node -1 (localhost/127.0.0.1:9092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient)\n",
      "[2022-06-09 01:49:15,050] WARN [Consumer clientId=console-consumer, groupId=console-consumer-74275] Bootstrap broker localhost:9092 (id: -1 rack: null) disconnected (org.apache.kafka.clients.NetworkClient)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 60:>                                                         (0 + 3) / 3]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022-06-09 01:49:16,209] WARN [Consumer clientId=console-consumer, groupId=console-consumer-74275] Connection to node -1 (localhost/127.0.0.1:9092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient)\n",
      "[2022-06-09 01:49:16,209] WARN [Consumer clientId=console-consumer, groupId=console-consumer-74275] Bootstrap broker localhost:9092 (id: -1 rack: null) disconnected (org.apache.kafka.clients.NetworkClient)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022-06-09 01:49:17,219] WARN [Consumer clientId=console-consumer, groupId=console-consumer-74275] Connection to node -1 (localhost/127.0.0.1:9092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient)\n",
      "[2022-06-09 01:49:17,220] WARN [Consumer clientId=console-consumer, groupId=console-consumer-74275] Bootstrap broker localhost:9092 (id: -1 rack: null) disconnected (org.apache.kafka.clients.NetworkClient)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 61:>                                                         (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022-06-09 01:49:18,275] WARN [Consumer clientId=console-consumer, groupId=console-consumer-74275] Connection to node -1 (localhost/127.0.0.1:9092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient)\n",
      "[2022-06-09 01:49:18,275] WARN [Consumer clientId=console-consumer, groupId=console-consumer-74275] Bootstrap broker localhost:9092 (id: -1 rack: null) disconnected (org.apache.kafka.clients.NetworkClient)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n",
      "Processed a total of 0 messages\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 65:>                                                         (0 + 3) / 3]\r"
     ]
    }
   ],
   "source": [
    "!kafka-console-consumer --bootstrap-server localhost:9092 --topic predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7ae91c3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/06/09 01:49:25 ERROR WriteToDataSourceV2Exec: Data source write support org.apache.spark.sql.execution.streaming.sources.MicroBatchWrite@61cf7630 is aborting.\n",
      "22/06/09 01:49:25 ERROR WriteToDataSourceV2Exec: Data source write support org.apache.spark.sql.execution.streaming.sources.MicroBatchWrite@61cf7630 aborted.\n",
      "22/06/09 01:49:25 ERROR Utils: Aborting task\n",
      "org.apache.spark.TaskKilledException\n",
      "\tat org.apache.spark.TaskContextImpl.killTaskIfInterrupted(TaskContextImpl.scala:216)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:36)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:513)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.$anonfun$run$1(WriteToDataSourceV2Exec.scala:412)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1496)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:457)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:358)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "22/06/09 01:49:25 ERROR DataWritingSparkTask: Aborting commit for partition 2 (task 80, attempt 0, stage 65.0)\n",
      "22/06/09 01:49:25 ERROR DataWritingSparkTask: Aborted commit for partition 2 (task 80, attempt 0, stage 65.0)\n",
      "22/06/09 01:49:25 ERROR Utils: Aborting task\n",
      "org.apache.spark.TaskKilledException\n",
      "\tat org.apache.spark.TaskContextImpl.killTaskIfInterrupted(TaskContextImpl.scala:216)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:36)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:513)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.$anonfun$run$1(WriteToDataSourceV2Exec.scala:412)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1496)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:457)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:358)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "22/06/09 01:49:25 ERROR DataWritingSparkTask: Aborting commit for partition 0 (task 78, attempt 0, stage 65.0)\n",
      "22/06/09 01:49:25 ERROR DataWritingSparkTask: Aborted commit for partition 0 (task 78, attempt 0, stage 65.0)\n",
      "22/06/09 01:49:25 ERROR Utils: Aborting task\n",
      "org.apache.spark.TaskKilledException\n",
      "\tat org.apache.spark.TaskContextImpl.killTaskIfInterrupted(TaskContextImpl.scala:216)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:36)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:513)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.$anonfun$run$1(WriteToDataSourceV2Exec.scala:412)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1496)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:457)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:358)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "22/06/09 01:49:25 ERROR DataWritingSparkTask: Aborting commit for partition 1 (task 79, attempt 0, stage 65.0)\n",
      "22/06/09 01:49:25 ERROR DataWritingSparkTask: Aborted commit for partition 1 (task 79, attempt 0, stage 65.0)\n"
     ]
    }
   ],
   "source": [
    "query.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0548163-9ba2-4b3d-8e6d-3db8ef943a6c",
   "metadata": {},
   "source": [
    "### Inserting data stream into PostgreSQL database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9947c7ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a PostgreSQL database with Docker\n",
    "!docker run -d -e POSTGRES_PASSWORD=postgres -p 5432:5432 --name postgres postgres:11.7-alpine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88640a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get PostgreSQL logs\n",
    "!docker logs postgres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc858ec",
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "%%sql postgresql://postgres:postgres@localhost:5432/postgres\n",
    "\n",
    "CREATE TABLE predictions (\n",
    "\tid DECIMAL(38, 0),\n",
    "\tprobability DOUBLE PRECISION,\n",
    "\tprocessed_at TIMESTAMP\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2d2a08c6-d78e-4f90-9954-7d83b6ac36bb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-06T17:04:46.631436Z",
     "iopub.status.busy": "2022-06-06T17:04:46.630914Z",
     "iopub.status.idle": "2022-06-06T17:04:46.789470Z",
     "shell.execute_reply": "2022-06-06T17:04:46.788754Z",
     "shell.execute_reply.started": "2022-06-06T17:04:46.631413Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "def foreach_batch_function(df, epoch_id):\n",
    "\n",
    "    df.write.format(\"jdbc\").option(\n",
    "        \"url\", \"jdbc:postgresql://localhost:5432/postgres\"\n",
    "    ).option(\"driver\", \"org.postgresql.Driver\").option(\"dbtable\", \"predictions\").option(\n",
    "        \"user\", \"postgres\"\n",
    "    ).option(\n",
    "        \"password\", \"postgres\"\n",
    "    ).mode(\n",
    "        \"append\"\n",
    "    ).save()\n",
    "\n",
    "query = results_postgres.writeStream.foreachBatch(foreach_batch_function).option(\n",
    "    \"checkpointLocation\", \"checkpointLocation\"\n",
    ").outputMode(\"update\").start()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ea67771b",
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 rows affected.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <tr>\n",
       "        <th>count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>126</td>\n",
       "    </tr>\n",
       "</table>"
      ],
      "text/plain": [
       "[(126,)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "%%sql postgresql://postgres:postgres@localhost:5432/postgres\n",
    "\n",
    "SELECT COUNT(*) FROM PREDICTIONS;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ccbc9c4",
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "%%sql postgresql://postgres:postgres@localhost:5432/postgres\n",
    "\n",
    "SELECT\n",
    "\t*\n",
    "FROM \n",
    "\tPREDICTIONS\n",
    "ORDER BY\n",
    "\tPROCESSED_AT DESC;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaebcb0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop data streams\n",
    "query.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a99c34e0-0469-44e3-b480-7710bb164872",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create Temp View\n",
    "# df.createOrReplaceTempView(\"dataframe\")\n",
    "\n",
    "# # Apply UDF in SQL query.\n",
    "# resultDF = spark.sql(\"select predict(*) as up_down_udf from dataframe\")\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "23290619d6f4f5ecea54fd3814d63b8a6a2d2c019f5870989b08cbcfb848aa36"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 ('spark-ml-training-Z1-rjbZ7-py3.9')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
