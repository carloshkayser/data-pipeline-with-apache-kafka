{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57597b01",
   "metadata": {},
   "source": [
    "# Apache Spark ML Training\n",
    "\n",
    "Dataset: https://www.kaggle.com/competitions/avazu-ctr-prediction/overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d10186c7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-06T03:13:19.900026Z",
     "iopub.status.busy": "2022-06-06T03:13:19.899189Z",
     "iopub.status.idle": "2022-06-06T03:13:20.594042Z",
     "shell.execute_reply": "2022-06-06T03:13:20.593325Z",
     "shell.execute_reply.started": "2022-06-06T03:13:19.899948Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PySpark 3.2.1\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import __version__\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# import pyspark.pandas as ps\n",
    "import pandas as pd\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "print(\"PySpark\", __version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9da1ced5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start Spark Standalone Cluster\n",
    "# $SPARK_HOME/sbin/start-all.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "95c219f9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-06T03:13:20.596112Z",
     "iopub.status.busy": "2022-06-06T03:13:20.595346Z",
     "iopub.status.idle": "2022-06-06T03:13:25.120995Z",
     "shell.execute_reply": "2022-06-06T03:13:25.120254Z",
     "shell.execute_reply.started": "2022-06-06T03:13:20.596080Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/06/15 00:06:20 WARN Utils: Your hostname, carloshkayser resolves to a loopback address: 127.0.1.1; using 10.32.45.215 instead (on interface ens160)\n",
      "22/06/15 00:06:20 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/kayser/spark-3.2.1-bin-hadoop3.2/jars/spark-unsafe_2.12-3.2.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/06/15 00:06:21 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "22/06/15 00:06:22 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.32.45.215:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>spark://carloshkayser:7077</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Spark ML</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f2bc9c1e610>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Spark ML\") \\\n",
    "    .master(\"spark://carloshkayser:7077\") \\\n",
    "    .config(\"spark.executor.memory\", \"24g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "19bcdcbd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-06T03:13:25.127729Z",
     "iopub.status.busy": "2022-06-06T03:13:25.127306Z",
     "iopub.status.idle": "2022-06-06T03:13:25.149472Z",
     "shell.execute_reply": "2022-06-06T03:13:25.148788Z",
     "shell.execute_reply.started": "2022-06-06T03:13:25.127700Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, FloatType, StringType, LongType, IntegerType, DoubleType\n",
    "\n",
    "schema = StructType([\n",
    "    StructField('id', LongType(), True),\n",
    "    StructField(\"click\", FloatType(), True),\n",
    "    StructField(\"hour\", IntegerType(), True),\n",
    "    StructField(\"C1\", IntegerType(), True),\n",
    "    StructField(\"banner_pos\", IntegerType(), True),\n",
    "    StructField(\"site_id\", StringType(), True),\n",
    "    StructField(\"site_domain\", StringType(), True),\n",
    "    StructField(\"site_category\", StringType(), True),\n",
    "    StructField(\"app_id\", StringType(), True),\n",
    "    StructField(\"app_domain\", StringType(), True),\n",
    "    StructField(\"app_category\", StringType(), True),\n",
    "    StructField(\"device_id\", StringType(), True),\n",
    "    StructField(\"device_ip\", StringType(), True),\n",
    "    StructField(\"device_model\", StringType(), True),\n",
    "    StructField(\"device_type\", IntegerType(), True),\n",
    "    StructField(\"device_conn_type\", IntegerType(), True),\n",
    "    StructField(\"C14\", IntegerType(), True),\n",
    "    StructField(\"C15\", IntegerType(), True),\n",
    "    StructField(\"C16\", IntegerType(), True),\n",
    "    StructField(\"C17\", IntegerType(), True),\n",
    "    StructField(\"C18\", IntegerType(), True),\n",
    "    StructField(\"C19\", IntegerType(), True),\n",
    "    StructField(\"C20\", IntegerType(), True),\n",
    "    StructField(\"C21\", IntegerType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a717f726",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-06T03:13:29.419799Z",
     "iopub.status.busy": "2022-06-06T03:13:29.419007Z",
     "iopub.status.idle": "2022-06-06T03:13:31.274847Z",
     "shell.execute_reply": "2022-06-06T03:13:31.274158Z",
     "shell.execute_reply.started": "2022-06-06T03:13:29.419769Z"
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .schema(schema) \\\n",
    "    .load('../dataset/click-through-rate-prediction/train.gz')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2cdce35e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-06T03:13:58.919551Z",
     "iopub.status.busy": "2022-06-06T03:13:58.918766Z",
     "iopub.status.idle": "2022-06-06T03:13:59.011144Z",
     "shell.execute_reply": "2022-06-06T03:13:59.010437Z",
     "shell.execute_reply.started": "2022-06-06T03:13:58.919519Z"
    }
   },
   "outputs": [],
   "source": [
    "df = df.na.drop()\n",
    "\n",
    "df = df.limit(1000000)\n",
    "\n",
    "df = df.withColumnRenamed(\"click\", \"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "52be6e5c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-06T03:13:59.351028Z",
     "iopub.status.busy": "2022-06-06T03:13:59.350293Z",
     "iopub.status.idle": "2022-06-06T03:15:31.414309Z",
     "shell.execute_reply": "2022-06-06T03:15:31.413260Z",
     "shell.execute_reply.started": "2022-06-06T03:13:59.350999Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/06/15 00:06:43 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "22/06/15 00:06:58 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "22/06/15 00:07:13 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "22/06/15 00:07:28 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "22/06/15 00:07:43 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "22/06/15 00:07:58 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "22/06/15 00:08:13 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "22/06/15 00:08:28 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "22/06/15 00:08:43 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "22/06/15 00:08:58 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "22/06/15 00:09:13 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "22/06/15 00:09:28 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "22/06/15 00:09:43 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "22/06/15 00:09:58 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "22/06/15 00:10:13 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "22/06/15 00:10:28 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "22/06/15 00:10:43 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "22/06/15 00:10:58 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "22/06/15 00:11:13 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "22/06/15 00:11:28 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "22/06/15 00:11:43 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "22/06/15 00:11:58 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "22/06/15 00:12:13 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "22/06/15 00:12:28 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "22/06/15 00:12:43 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "22/06/15 00:12:58 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "22/06/15 00:13:13 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "22/06/15 00:13:28 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "22/06/15 00:13:43 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "22/06/15 00:13:58 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "22/06/15 00:14:13 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1000000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "490a3395",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-06T03:15:31.417810Z",
     "iopub.status.busy": "2022-06-06T03:15:31.416054Z",
     "iopub.status.idle": "2022-06-06T03:15:31.456723Z",
     "shell.execute_reply": "2022-06-06T03:15:31.456006Z",
     "shell.execute_reply.started": "2022-06-06T03:15:31.417781Z"
    }
   },
   "outputs": [],
   "source": [
    "test_data, raw_training_data = df.randomSplit([0.3, 0.7])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c0d58ddb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-06T03:15:31.463289Z",
     "iopub.status.busy": "2022-06-06T03:15:31.462871Z",
     "iopub.status.idle": "2022-06-06T03:15:31.468958Z",
     "shell.execute_reply": "2022-06-06T03:15:31.468390Z",
     "shell.execute_reply.started": "2022-06-06T03:15:31.463265Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- label: float (nullable = true)\n",
      " |-- hour: integer (nullable = true)\n",
      " |-- C1: integer (nullable = true)\n",
      " |-- banner_pos: integer (nullable = true)\n",
      " |-- site_id: string (nullable = true)\n",
      " |-- site_domain: string (nullable = true)\n",
      " |-- site_category: string (nullable = true)\n",
      " |-- app_id: string (nullable = true)\n",
      " |-- app_domain: string (nullable = true)\n",
      " |-- app_category: string (nullable = true)\n",
      " |-- device_id: string (nullable = true)\n",
      " |-- device_ip: string (nullable = true)\n",
      " |-- device_model: string (nullable = true)\n",
      " |-- device_type: integer (nullable = true)\n",
      " |-- device_conn_type: integer (nullable = true)\n",
      " |-- C14: integer (nullable = true)\n",
      " |-- C15: integer (nullable = true)\n",
      " |-- C16: integer (nullable = true)\n",
      " |-- C17: integer (nullable = true)\n",
      " |-- C18: integer (nullable = true)\n",
      " |-- C19: integer (nullable = true)\n",
      " |-- C20: integer (nullable = true)\n",
      " |-- C21: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "raw_training_data.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "19657367",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-06T03:15:31.474681Z",
     "iopub.status.busy": "2022-06-06T03:15:31.474444Z",
     "iopub.status.idle": "2022-06-06T03:15:31.571656Z",
     "shell.execute_reply": "2022-06-06T03:15:31.570824Z",
     "shell.execute_reply.started": "2022-06-06T03:15:31.474660Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.feature import OneHotEncoder\n",
    "from pyspark.ml.feature import OneHotEncoder\n",
    "from pyspark.ml.feature import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "688e00a4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-06T03:15:31.573256Z",
     "iopub.status.busy": "2022-06-06T03:15:31.572991Z",
     "iopub.status.idle": "2022-06-06T03:15:31.636737Z",
     "shell.execute_reply": "2022-06-06T03:15:31.635966Z",
     "shell.execute_reply.started": "2022-06-06T03:15:31.573232Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create the logistic regression model\n",
    "lr = LogisticRegression(maxIter=10, regParam= 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "679a90f3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-06T03:15:31.637984Z",
     "iopub.status.busy": "2022-06-06T03:15:31.637746Z",
     "iopub.status.idle": "2022-06-06T03:15:31.674696Z",
     "shell.execute_reply": "2022-06-06T03:15:31.673958Z",
     "shell.execute_reply.started": "2022-06-06T03:15:31.637961Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create a StringIndexer\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "cols = []\n",
    "pipeline_stages = []\n",
    "feature_columns = []\n",
    "\n",
    "for name, type in raw_training_data.dtypes:\n",
    "    if type == \"string\":\n",
    "        feature_columns.append(f\"{name}Index\")\n",
    "        pipeline_stages.append(StringIndexer(inputCol=name, outputCol=f\"{name}Index\", handleInvalid=\"skip\"))\n",
    "    \n",
    "        cols.append(f\"{name}Index\")\n",
    "    \n",
    "    else:\n",
    "        cols.append(name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "680047ca",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-06T03:15:31.675949Z",
     "iopub.status.busy": "2022-06-06T03:15:31.675706Z",
     "iopub.status.idle": "2022-06-06T03:15:31.686516Z",
     "shell.execute_reply": "2022-06-06T03:15:31.685823Z",
     "shell.execute_reply.started": "2022-06-06T03:15:31.675928Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create a one hot encoder\n",
    "feature_columns = ['site_idIndex', 'site_domainIndex', 'site_categoryIndex', 'app_idIndex', 'app_domainIndex', 'app_categoryIndex', 'device_idIndex', 'device_ipIndex', 'device_modelIndex']\n",
    "output_ohe_columns = ['site_id_ohe', 'site_domain_ohe', 'site_category_ohe', 'app_id_ohe', 'app_domain_ohe', 'app_category_ohe', 'device_id_ohe', 'device_ip_ohe', 'device_model_ohe']\n",
    "\n",
    "ohe = OneHotEncoder(inputCols = feature_columns, outputCols = output_ohe_columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1bcaedc0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-06T03:15:31.689098Z",
     "iopub.status.busy": "2022-06-06T03:15:31.688604Z",
     "iopub.status.idle": "2022-06-06T03:15:31.703989Z",
     "shell.execute_reply": "2022-06-06T03:15:31.703193Z",
     "shell.execute_reply.started": "2022-06-06T03:15:31.689073Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import MinMaxScaler\n",
    "\n",
    "# Input list for scaling\n",
    "inputs = [\"hour\", \"C1\", \"banner_pos\", \"device_type\", \"device_conn_type\", \"C14\", \"C15\", \"C16\", \"C17\", \"C18\", \"C19\", \"C20\", \"C21\"]\n",
    "\n",
    "# We scale our inputs\n",
    "assembler1 = VectorAssembler(inputCols=inputs, outputCol=\"features_scaled1\")\n",
    "scaler = MinMaxScaler(inputCol=\"features_scaled1\", outputCol=\"features_scaled\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6ab4af82",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-06T03:15:31.705073Z",
     "iopub.status.busy": "2022-06-06T03:15:31.704853Z",
     "iopub.status.idle": "2022-06-06T03:15:31.710536Z",
     "shell.execute_reply": "2022-06-06T03:15:31.709845Z",
     "shell.execute_reply.started": "2022-06-06T03:15:31.705053Z"
    }
   },
   "outputs": [],
   "source": [
    "# We create a second assembler for the encoded columns.\n",
    "assembler2 = VectorAssembler(\n",
    "  inputCols=['features_scaled'] + output_ohe_columns, outputCol=\"features\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fe0fe8e2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-06T03:15:31.712173Z",
     "iopub.status.busy": "2022-06-06T03:15:31.711829Z",
     "iopub.status.idle": "2022-06-06T03:19:05.717066Z",
     "shell.execute_reply": "2022-06-06T03:19:05.716035Z",
     "shell.execute_reply.started": "2022-06-06T03:15:31.712151Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/06/15 00:17:06 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
      "22/06/15 00:17:27 WARN DAGScheduler: Broadcasting large task binary with size 9.5 MiB\n",
      "22/06/15 00:17:31 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "22/06/15 00:17:47 WARN DAGScheduler: Broadcasting large task binary with size 9.6 MiB\n",
      "22/06/15 00:18:36 WARN DAGScheduler: Broadcasting large task binary with size 35.5 MiB\n",
      "22/06/15 00:18:51 WARN DAGScheduler: Broadcasting large task binary with size 35.5 MiB\n",
      "22/06/15 00:19:05 WARN InstanceBuilder$NativeBLAS: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "22/06/15 00:19:05 WARN InstanceBuilder$NativeBLAS: Failed to load implementation from:dev.ludovic.netlib.blas.ForeignLinkerBLAS\n",
      "22/06/15 00:19:05 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS\n",
      "22/06/15 00:19:05 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS\n",
      "22/06/15 00:19:06 WARN DAGScheduler: Broadcasting large task binary with size 35.5 MiB\n",
      "22/06/15 00:19:09 WARN DAGScheduler: Broadcasting large task binary with size 35.5 MiB\n",
      "22/06/15 00:19:11 WARN DAGScheduler: Broadcasting large task binary with size 35.5 MiB\n",
      "22/06/15 00:19:14 WARN DAGScheduler: Broadcasting large task binary with size 35.5 MiB\n",
      "22/06/15 00:19:16 WARN DAGScheduler: Broadcasting large task binary with size 35.5 MiB\n",
      "22/06/15 00:19:19 WARN DAGScheduler: Broadcasting large task binary with size 35.5 MiB\n",
      "22/06/15 00:19:21 WARN DAGScheduler: Broadcasting large task binary with size 35.5 MiB\n",
      "22/06/15 00:19:24 WARN DAGScheduler: Broadcasting large task binary with size 35.5 MiB\n",
      "22/06/15 00:19:26 WARN DAGScheduler: Broadcasting large task binary with size 35.5 MiB\n",
      "22/06/15 00:19:28 WARN DAGScheduler: Broadcasting large task binary with size 35.5 MiB\n",
      "22/06/15 00:20:28 WARN DAGScheduler: Broadcasting large task binary with size 38.4 MiB\n",
      "[Stage 63:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+----------+\n",
      "|label|         probability|prediction|\n",
      "+-----+--------------------+----------+\n",
      "|  0.0|[0.73177397072966...|       0.0|\n",
      "|  0.0|[0.99430990977573...|       0.0|\n",
      "|  0.0|[0.99251657844327...|       0.0|\n",
      "|  0.0|[0.98793467334425...|       0.0|\n",
      "|  0.0|[0.99140607237181...|       0.0|\n",
      "|  1.0|[0.05275639135396...|       1.0|\n",
      "|  0.0|[0.98696397007111...|       0.0|\n",
      "|  0.0|[0.99029424100199...|       0.0|\n",
      "|  0.0|[0.72063436022881...|       0.0|\n",
      "|  0.0|[0.98635162555681...|       0.0|\n",
      "|  0.0|[0.74332604504987...|       0.0|\n",
      "|  0.0|[0.99736128272305...|       0.0|\n",
      "|  0.0|[0.99117274337270...|       0.0|\n",
      "|  1.0|[0.02297802281914...|       1.0|\n",
      "|  0.0|[0.96260789092916...|       0.0|\n",
      "|  1.0|[0.31539757863386...|       1.0|\n",
      "|  0.0|[0.98475567498695...|       0.0|\n",
      "|  0.0|[0.98210009314176...|       0.0|\n",
      "|  0.0|[0.98117998079269...|       0.0|\n",
      "|  0.0|[0.99013585648942...|       0.0|\n",
      "+-----+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# Create stages list\n",
    "myStages = pipeline_stages + [assembler1, scaler, ohe, assembler2, lr]\n",
    "\n",
    "# Set up the pipeline\n",
    "pipeline = Pipeline(stages= myStages)\n",
    "\n",
    "# We fit the model using the training data.\n",
    "pModel = pipeline.fit(raw_training_data)\n",
    "\n",
    "# We transform the data.\n",
    "trainingPred = pModel.transform(raw_training_data)\n",
    "\n",
    "# # We select the actual label, probability and predictions\n",
    "trainingPred.select('label', 'probability', 'prediction').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "635bc295",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-06T03:19:05.718666Z",
     "iopub.status.busy": "2022-06-06T03:19:05.718379Z",
     "iopub.status.idle": "2022-06-06T03:19:14.400965Z",
     "shell.execute_reply": "2022-06-06T03:19:14.398554Z",
     "shell.execute_reply.started": "2022-06-06T03:19:05.718641Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/06/15 00:20:33 WARN TaskSetManager: Stage 90 contains a task of very large size (1188 KiB). The maximum recommended task size is 1000 KiB.\n",
      "22/06/15 00:20:34 WARN TaskSetManager: Stage 94 contains a task of very large size (4669 KiB). The maximum recommended task size is 1000 KiB.\n",
      "22/06/15 00:20:35 WARN TaskSetManager: Stage 112 contains a task of very large size (3040 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    }
   ],
   "source": [
    "pModel.save(\"model/spark-logistic-regression-model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "461169c5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-06T03:19:14.402149Z",
     "iopub.status.busy": "2022-06-06T03:19:14.401915Z",
     "iopub.status.idle": "2022-06-06T03:19:16.699782Z",
     "shell.execute_reply": "2022-06-06T03:19:16.699095Z",
     "shell.execute_reply.started": "2022-06-06T03:19:14.402127Z"
    }
   },
   "outputs": [],
   "source": [
    "pred = pModel.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d3da2151",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-06T03:19:16.701451Z",
     "iopub.status.busy": "2022-06-06T03:19:16.700769Z",
     "iopub.status.idle": "2022-06-06T03:19:31.989565Z",
     "shell.execute_reply": "2022-06-06T03:19:31.988936Z",
     "shell.execute_reply.started": "2022-06-06T03:19:16.701420Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/06/15 00:21:00 WARN DAGScheduler: Broadcasting large task binary with size 38.4 MiB\n",
      "[Stage 117:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy = 0.832906 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "\n",
    "accuracy = evaluator.evaluate(pred)\n",
    "\n",
    "print(\"Train Accuracy = %g \" % (accuracy))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84dd6804",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "23290619d6f4f5ecea54fd3814d63b8a6a2d2c019f5870989b08cbcfb848aa36"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
