{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d10186c7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-06T03:13:19.900026Z",
     "iopub.status.busy": "2022-06-06T03:13:19.899189Z",
     "iopub.status.idle": "2022-06-06T03:13:20.594042Z",
     "shell.execute_reply": "2022-06-06T03:13:20.593325Z",
     "shell.execute_reply.started": "2022-06-06T03:13:19.899948Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PySpark 3.2.1\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import __version__\n",
    "\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# import pyspark.pandas as ps\n",
    "import pandas as pd\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "print(\"PySpark\", __version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95c219f9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-06T03:13:20.596112Z",
     "iopub.status.busy": "2022-06-06T03:13:20.595346Z",
     "iopub.status.idle": "2022-06-06T03:13:25.120995Z",
     "shell.execute_reply": "2022-06-06T03:13:25.120254Z",
     "shell.execute_reply.started": "2022-06-06T03:13:20.596080Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/06/06 03:13:21 WARN Utils: Your hostname, carloshkayser resolves to a loopback address: 127.0.1.1; using 10.32.45.215 instead (on interface ens160)\n",
      "22/06/06 03:13:21 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/kayser/spark-3.2.1-bin-hadoop3.2/jars/spark-unsafe_2.12-3.2.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/06/06 03:13:22 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "22/06/06 03:13:23 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.32.45.215:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>spark://carloshkayser:7077</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f8ab5b56bb0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# spark = SparkSession.builder \\\n",
    "    # .master('local[*]') \\\n",
    "#     .appName(\"Spark ML\") \\\n",
    "#     .master(\"spark://10.32.45.215:7077\") \\\n",
    "#     .config(\"spark.executor.memory\", \"24g\") \\\n",
    "#     .getOrCreate()\n",
    "  # .config(\"spark.dynamicAllocation.enabled\", \"true\") \\\n",
    "  # .config(\"spark.shuffle.service.enabled\", \"true\") \\\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "  .master(\"spark://carloshkayser:7077\") \\\n",
    "  .config(\"spark.driver.memory\", \"6g\") \\\n",
    "  .config(\"spark.executor.instances\", \"4\") \\\n",
    "  .config(\"spark.executor.memory\", \"4g\") \\\n",
    "  .config(\"spark.executor.cores\", \"2\") \\\n",
    "  .getOrCreate()\n",
    "\n",
    "# .config(\"spark.driver.memory\", \"24g\") \\\n",
    "# .config(\"spark.kryoserializer.buffer.max\", \"512m\") \\\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf7a9c92",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-06T03:13:25.122963Z",
     "iopub.status.busy": "2022-06-06T03:13:25.122425Z",
     "iopub.status.idle": "2022-06-06T03:13:25.126368Z",
     "shell.execute_reply": "2022-06-06T03:13:25.125724Z",
     "shell.execute_reply.started": "2022-06-06T03:13:25.122934Z"
    }
   },
   "outputs": [],
   "source": [
    "# from pyspark import SparkContext, SparkConf\n",
    "# from pyspark.sql import SparkSession\n",
    "\n",
    "# sparkConf = SparkConf()\n",
    "\n",
    "# # sparkConf.setMaster(\"k8s://kubernetes.default.svc.cluster.local\")\n",
    "# sparkConf.setMaster(\"k8s://https://192.168.49.2:8443\") # minikube ip\n",
    "\n",
    "# sparkConf.setAppName(\"KUBERNETES-IS-AWESOME\")\n",
    "# sparkConf.set(\"spark.kubernetes.container.image\", \"kayser/spark-py:3.2.1-hadoop-3.2.0\")\n",
    "# sparkConf.set(\"spark.kubernetes.namespace\", \"spark\")\n",
    "\n",
    "# sparkConf.set(\"spark.executor.instances\", \"2\")\n",
    "# sparkConf.set(\"spark.executor.memory\", \"4g\")\n",
    "# sparkConf.set(\"spark.executor.cores\", \"2\")\n",
    "# sparkConf.set(\"spark.driver.blockManager.port\", \"7777\")\n",
    "# sparkConf.set(\"spark.driver.port\", \"2222\")\n",
    "# # sparkConf.set(\"spark.driver.host\", \"jupyter.spark.svc.cluster.local\")\n",
    "# # sparkConf.set(\"spark.driver.bindAddress\", \"0.0.0.0\")\n",
    "\n",
    "# spark = SparkSession.builder.config(conf=sparkConf).getOrCreate()\n",
    "# spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "19bcdcbd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-06T03:13:25.127729Z",
     "iopub.status.busy": "2022-06-06T03:13:25.127306Z",
     "iopub.status.idle": "2022-06-06T03:13:25.149472Z",
     "shell.execute_reply": "2022-06-06T03:13:25.148788Z",
     "shell.execute_reply.started": "2022-06-06T03:13:25.127700Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, FloatType, StringType, LongType, IntegerType, DoubleType\n",
    "\n",
    "schema = StructType([\n",
    "    StructField('id', LongType(), True),\n",
    "    StructField(\"click\", FloatType(), True),\n",
    "    StructField(\"hour\", IntegerType(), True),\n",
    "    StructField(\"C1\", IntegerType(), True),\n",
    "    StructField(\"banner_pos\", IntegerType(), True),\n",
    "    StructField(\"site_id\", StringType(), True),\n",
    "    StructField(\"site_domain\", StringType(), True),\n",
    "    StructField(\"site_category\", StringType(), True),\n",
    "    StructField(\"app_id\", StringType(), True),\n",
    "    StructField(\"app_domain\", StringType(), True),\n",
    "    StructField(\"app_category\", StringType(), True),\n",
    "    StructField(\"device_id\", StringType(), True),\n",
    "    StructField(\"device_ip\", StringType(), True),\n",
    "    StructField(\"device_model\", StringType(), True),\n",
    "    StructField(\"device_type\", IntegerType(), True),\n",
    "    StructField(\"device_conn_type\", IntegerType(), True),\n",
    "    StructField(\"C14\", IntegerType(), True),\n",
    "    StructField(\"C15\", IntegerType(), True),\n",
    "    StructField(\"C16\", IntegerType(), True),\n",
    "    StructField(\"C17\", IntegerType(), True),\n",
    "    StructField(\"C18\", IntegerType(), True),\n",
    "    StructField(\"C19\", IntegerType(), True),\n",
    "    StructField(\"C20\", IntegerType(), True),\n",
    "    StructField(\"C21\", IntegerType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a717f726",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-06T03:13:29.419799Z",
     "iopub.status.busy": "2022-06-06T03:13:29.419007Z",
     "iopub.status.idle": "2022-06-06T03:13:31.274847Z",
     "shell.execute_reply": "2022-06-06T03:13:31.274158Z",
     "shell.execute_reply.started": "2022-06-06T03:13:29.419769Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load training data\n",
    "# training = spark.read.format(\"csv\") \\\n",
    "#     .option(\"header\", \"true\") \\\n",
    "#     .option(\"inferSchema\", \"true\") \\\n",
    "#     .load('../dataset/click-through-rate-prediction/train.gz')\n",
    "\n",
    "df = spark.read.format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .schema(schema) \\\n",
    "    .load('../dataset/click-through-rate-prediction/train.gz')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3e730eb8-d4f5-4aea-a2bb-a3946f95010d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-06T03:13:57.115478Z",
     "iopub.status.busy": "2022-06-06T03:13:57.112490Z",
     "iopub.status.idle": "2022-06-06T03:13:57.150153Z",
     "shell.execute_reply": "2022-06-06T03:13:57.149443Z",
     "shell.execute_reply.started": "2022-06-06T03:13:57.115341Z"
    }
   },
   "outputs": [],
   "source": [
    "df = df.limit(1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2cdce35e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-06T03:13:58.919551Z",
     "iopub.status.busy": "2022-06-06T03:13:58.918766Z",
     "iopub.status.idle": "2022-06-06T03:13:59.011144Z",
     "shell.execute_reply": "2022-06-06T03:13:59.010437Z",
     "shell.execute_reply.started": "2022-06-06T03:13:58.919519Z"
    }
   },
   "outputs": [],
   "source": [
    "df = df.na.drop() \n",
    "\n",
    "df = df.withColumnRenamed(\"click\", \"label\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "52be6e5c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-06T03:13:59.351028Z",
     "iopub.status.busy": "2022-06-06T03:13:59.350293Z",
     "iopub.status.idle": "2022-06-06T03:15:31.414309Z",
     "shell.execute_reply": "2022-06-06T03:15:31.413260Z",
     "shell.execute_reply.started": "2022-06-06T03:13:59.350999Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/06/06 03:14:15 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "22/06/06 03:14:30 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "22/06/06 03:14:45 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "22/06/06 03:15:00 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "22/06/06 03:15:15 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "469294"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "490a3395",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-06T03:15:31.417810Z",
     "iopub.status.busy": "2022-06-06T03:15:31.416054Z",
     "iopub.status.idle": "2022-06-06T03:15:31.456723Z",
     "shell.execute_reply": "2022-06-06T03:15:31.456006Z",
     "shell.execute_reply.started": "2022-06-06T03:15:31.417781Z"
    }
   },
   "outputs": [],
   "source": [
    "test_data, raw_training_data = df.randomSplit([0.3, 0.7])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f74496ad",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-06T03:15:31.458557Z",
     "iopub.status.busy": "2022-06-06T03:15:31.457867Z",
     "iopub.status.idle": "2022-06-06T03:15:31.461265Z",
     "shell.execute_reply": "2022-06-06T03:15:31.460692Z",
     "shell.execute_reply.started": "2022-06-06T03:15:31.458529Z"
    }
   },
   "outputs": [],
   "source": [
    "# raw_training_data = training.limit(1000)\n",
    "\n",
    "# raw_training_data = raw_training_data.na.drop() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c0d58ddb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-06T03:15:31.463289Z",
     "iopub.status.busy": "2022-06-06T03:15:31.462871Z",
     "iopub.status.idle": "2022-06-06T03:15:31.468958Z",
     "shell.execute_reply": "2022-06-06T03:15:31.468390Z",
     "shell.execute_reply.started": "2022-06-06T03:15:31.463265Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- label: float (nullable = true)\n",
      " |-- hour: integer (nullable = true)\n",
      " |-- C1: integer (nullable = true)\n",
      " |-- banner_pos: integer (nullable = true)\n",
      " |-- site_id: string (nullable = true)\n",
      " |-- site_domain: string (nullable = true)\n",
      " |-- site_category: string (nullable = true)\n",
      " |-- app_id: string (nullable = true)\n",
      " |-- app_domain: string (nullable = true)\n",
      " |-- app_category: string (nullable = true)\n",
      " |-- device_id: string (nullable = true)\n",
      " |-- device_ip: string (nullable = true)\n",
      " |-- device_model: string (nullable = true)\n",
      " |-- device_type: integer (nullable = true)\n",
      " |-- device_conn_type: integer (nullable = true)\n",
      " |-- C14: integer (nullable = true)\n",
      " |-- C15: integer (nullable = true)\n",
      " |-- C16: integer (nullable = true)\n",
      " |-- C17: integer (nullable = true)\n",
      " |-- C18: integer (nullable = true)\n",
      " |-- C19: integer (nullable = true)\n",
      " |-- C20: integer (nullable = true)\n",
      " |-- C21: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# raw_training_data = raw_training_data.withColumnRenamed(\"click\", \"label\")\n",
    "\n",
    "raw_training_data.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c3b1ff7d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-06T03:15:31.470488Z",
     "iopub.status.busy": "2022-06-06T03:15:31.469857Z",
     "iopub.status.idle": "2022-06-06T03:15:31.473342Z",
     "shell.execute_reply": "2022-06-06T03:15:31.472511Z",
     "shell.execute_reply.started": "2022-06-06T03:15:31.470463Z"
    }
   },
   "outputs": [],
   "source": [
    "# # raw_training_data = raw_training_data.limit(1000)\n",
    "\n",
    "# raw_training_data = raw_training_data.withColumnRenamed(\"click\", \"label\")\n",
    "\n",
    "# raw_training_data = raw_training_data.na.drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "19657367",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-06T03:15:31.474681Z",
     "iopub.status.busy": "2022-06-06T03:15:31.474444Z",
     "iopub.status.idle": "2022-06-06T03:15:31.571656Z",
     "shell.execute_reply": "2022-06-06T03:15:31.570824Z",
     "shell.execute_reply.started": "2022-06-06T03:15:31.474660Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.feature import OneHotEncoder\n",
    "from pyspark.ml.feature import OneHotEncoder\n",
    "from pyspark.ml.feature import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "688e00a4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-06T03:15:31.573256Z",
     "iopub.status.busy": "2022-06-06T03:15:31.572991Z",
     "iopub.status.idle": "2022-06-06T03:15:31.636737Z",
     "shell.execute_reply": "2022-06-06T03:15:31.635966Z",
     "shell.execute_reply.started": "2022-06-06T03:15:31.573232Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create the logistic regression model\n",
    "lr = LogisticRegression(maxIter=10, regParam= 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "679a90f3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-06T03:15:31.637984Z",
     "iopub.status.busy": "2022-06-06T03:15:31.637746Z",
     "iopub.status.idle": "2022-06-06T03:15:31.674696Z",
     "shell.execute_reply": "2022-06-06T03:15:31.673958Z",
     "shell.execute_reply.started": "2022-06-06T03:15:31.637961Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create a StringIndexer\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "cols = []\n",
    "pipeline_stages = []\n",
    "feature_columns = []\n",
    "\n",
    "for name, type in raw_training_data.dtypes:\n",
    "    if type == \"string\":\n",
    "        feature_columns.append(f\"{name}Index\")\n",
    "        pipeline_stages.append(StringIndexer(inputCol=name, outputCol=f\"{name}Index\", handleInvalid=\"skip\"))\n",
    "    \n",
    "        cols.append(f\"{name}Index\")\n",
    "    \n",
    "    else:\n",
    "        cols.append(name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "680047ca",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-06T03:15:31.675949Z",
     "iopub.status.busy": "2022-06-06T03:15:31.675706Z",
     "iopub.status.idle": "2022-06-06T03:15:31.686516Z",
     "shell.execute_reply": "2022-06-06T03:15:31.685823Z",
     "shell.execute_reply.started": "2022-06-06T03:15:31.675928Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create a one hot encoder\n",
    "feature_columns = ['site_idIndex', 'site_domainIndex', 'site_categoryIndex', 'app_idIndex', 'app_domainIndex', 'app_categoryIndex', 'device_idIndex', 'device_ipIndex', 'device_modelIndex']\n",
    "output_ohe_columns = ['site_id_ohe', 'site_domain_ohe', 'site_category_ohe', 'app_id_ohe', 'app_domain_ohe', 'app_category_ohe', 'device_id_ohe', 'device_ip_ohe', 'device_model_ohe']\n",
    "\n",
    "ohe = OneHotEncoder(inputCols = feature_columns, outputCols = output_ohe_columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1bcaedc0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-06T03:15:31.689098Z",
     "iopub.status.busy": "2022-06-06T03:15:31.688604Z",
     "iopub.status.idle": "2022-06-06T03:15:31.703989Z",
     "shell.execute_reply": "2022-06-06T03:15:31.703193Z",
     "shell.execute_reply.started": "2022-06-06T03:15:31.689073Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import MinMaxScaler\n",
    "\n",
    "# Input list for scaling\n",
    "inputs = [\"hour\", \"C1\", \"banner_pos\", \"device_type\", \"device_conn_type\", \"C14\", \"C15\", \"C16\", \"C17\", \"C18\", \"C19\", \"C20\", \"C21\"]\n",
    "\n",
    "# We scale our inputs\n",
    "assembler1 = VectorAssembler(inputCols=inputs, outputCol=\"features_scaled1\")\n",
    "scaler = MinMaxScaler(inputCol=\"features_scaled1\", outputCol=\"features_scaled\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6ab4af82",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-06T03:15:31.705073Z",
     "iopub.status.busy": "2022-06-06T03:15:31.704853Z",
     "iopub.status.idle": "2022-06-06T03:15:31.710536Z",
     "shell.execute_reply": "2022-06-06T03:15:31.709845Z",
     "shell.execute_reply.started": "2022-06-06T03:15:31.705053Z"
    }
   },
   "outputs": [],
   "source": [
    "# We create a second assembler for the encoded columns.\n",
    "assembler2 = VectorAssembler(\n",
    "  inputCols=['features_scaled'] + output_ohe_columns, outputCol=\"features\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fe0fe8e2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-06T03:15:31.712173Z",
     "iopub.status.busy": "2022-06-06T03:15:31.711829Z",
     "iopub.status.idle": "2022-06-06T03:19:05.717066Z",
     "shell.execute_reply": "2022-06-06T03:19:05.716035Z",
     "shell.execute_reply.started": "2022-06-06T03:15:31.712151Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/06/06 03:17:07 WARN DAGScheduler: Broadcasting large task binary with size 1172.5 KiB\n",
      "22/06/06 03:17:23 WARN DAGScheduler: Broadcasting large task binary with size 4.9 MiB\n",
      "22/06/06 03:17:25 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "22/06/06 03:17:35 WARN DAGScheduler: Broadcasting large task binary with size 5.0 MiB\n",
      "22/06/06 03:18:03 WARN DAGScheduler: Broadcasting large task binary with size 18.4 MiB\n",
      "22/06/06 03:18:12 WARN DAGScheduler: Broadcasting large task binary with size 18.4 MiB\n",
      "22/06/06 03:18:20 WARN InstanceBuilder$NativeBLAS: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "22/06/06 03:18:20 WARN InstanceBuilder$NativeBLAS: Failed to load implementation from:dev.ludovic.netlib.blas.ForeignLinkerBLAS\n",
      "22/06/06 03:18:20 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS\n",
      "22/06/06 03:18:20 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS\n",
      "22/06/06 03:18:21 WARN DAGScheduler: Broadcasting large task binary with size 18.4 MiB\n",
      "22/06/06 03:18:22 WARN DAGScheduler: Broadcasting large task binary with size 18.4 MiB\n",
      "22/06/06 03:18:23 WARN DAGScheduler: Broadcasting large task binary with size 18.4 MiB\n",
      "22/06/06 03:18:24 WARN DAGScheduler: Broadcasting large task binary with size 18.4 MiB\n",
      "22/06/06 03:18:25 WARN DAGScheduler: Broadcasting large task binary with size 18.4 MiB\n",
      "22/06/06 03:18:26 WARN DAGScheduler: Broadcasting large task binary with size 18.4 MiB\n",
      "22/06/06 03:18:28 WARN DAGScheduler: Broadcasting large task binary with size 18.4 MiB\n",
      "22/06/06 03:18:29 WARN DAGScheduler: Broadcasting large task binary with size 18.4 MiB\n",
      "22/06/06 03:18:30 WARN DAGScheduler: Broadcasting large task binary with size 18.4 MiB\n",
      "22/06/06 03:18:31 WARN DAGScheduler: Broadcasting large task binary with size 18.4 MiB\n",
      "22/06/06 03:19:03 WARN DAGScheduler: Broadcasting large task binary with size 19.9 MiB\n",
      "[Stage 63:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+----------+\n",
      "|label|         probability|prediction|\n",
      "+-----+--------------------+----------+\n",
      "|  0.0|[0.99045397072254...|       0.0|\n",
      "|  0.0|[0.80807590059215...|       0.0|\n",
      "|  0.0|[0.98711788683391...|       0.0|\n",
      "|  0.0|[0.70260273564967...|       0.0|\n",
      "|  0.0|[0.87271516926943...|       0.0|\n",
      "|  1.0|[0.01903280412963...|       1.0|\n",
      "|  1.0|[0.05092559098070...|       1.0|\n",
      "|  1.0|[0.47611269367332...|       1.0|\n",
      "|  0.0|[0.98773308401171...|       0.0|\n",
      "|  0.0|[0.98117584756017...|       0.0|\n",
      "|  0.0|[0.98089943122165...|       0.0|\n",
      "|  0.0|[0.99139140112180...|       0.0|\n",
      "|  0.0|[0.97994919525048...|       0.0|\n",
      "|  0.0|[0.82804539057072...|       0.0|\n",
      "|  0.0|[0.48790869933666...|       1.0|\n",
      "|  0.0|[0.99032445620101...|       0.0|\n",
      "|  0.0|[0.99228023307818...|       0.0|\n",
      "|  0.0|[0.93538059880082...|       0.0|\n",
      "|  0.0|[0.98595734351784...|       0.0|\n",
      "|  0.0|[0.93120113551140...|       0.0|\n",
      "+-----+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# Create stages list\n",
    "myStages = pipeline_stages + [assembler1, scaler, ohe, assembler2, lr]\n",
    "\n",
    "# Set up the pipeline\n",
    "pipeline = Pipeline(stages= myStages)\n",
    "\n",
    "# We fit the model using the training data.\n",
    "pModel = pipeline.fit(raw_training_data)\n",
    "\n",
    "# We transform the data.\n",
    "trainingPred = pModel.transform(raw_training_data)\n",
    "\n",
    "# # We select the actual label, probability and predictions\n",
    "trainingPred.select('label', 'probability', 'prediction').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "635bc295",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-06T03:19:05.718666Z",
     "iopub.status.busy": "2022-06-06T03:19:05.718379Z",
     "iopub.status.idle": "2022-06-06T03:19:14.400965Z",
     "shell.execute_reply": "2022-06-06T03:19:14.398554Z",
     "shell.execute_reply.started": "2022-06-06T03:19:05.718641Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/06/06 03:19:11 WARN TaskSetManager: Stage 94 contains a task of very large size (2411 KiB). The maximum recommended task size is 1000 KiB.\n",
      "22/06/06 03:19:13 WARN TaskSetManager: Stage 112 contains a task of very large size (1565 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "pModel.save(\"model/spark-logistic-regression-model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "461169c5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-06T03:19:14.402149Z",
     "iopub.status.busy": "2022-06-06T03:19:14.401915Z",
     "iopub.status.idle": "2022-06-06T03:19:16.699782Z",
     "shell.execute_reply": "2022-06-06T03:19:16.699095Z",
     "shell.execute_reply.started": "2022-06-06T03:19:14.402127Z"
    }
   },
   "outputs": [],
   "source": [
    "pred = pModel.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d3da2151",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-06T03:19:16.701451Z",
     "iopub.status.busy": "2022-06-06T03:19:16.700769Z",
     "iopub.status.idle": "2022-06-06T03:19:31.989565Z",
     "shell.execute_reply": "2022-06-06T03:19:31.988936Z",
     "shell.execute_reply.started": "2022-06-06T03:19:16.701420Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/06/06 03:19:27 WARN DAGScheduler: Broadcasting large task binary with size 19.9 MiB\n",
      "[Stage 117:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy = 0.83162 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "\n",
    "accuracy = evaluator.evaluate(pred)\n",
    "\n",
    "print(\"Train Accuracy = %g \" % (accuracy))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d5e2275-d323-456c-acfd-15eeec16f8bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b790f7ad-0589-49a4-aaa6-799071b0bbc8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6e9b7fe0-4891-454c-8369-1839c499c65d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-06T04:17:19.281367Z",
     "iopub.status.busy": "2022-06-06T04:17:19.280968Z",
     "iopub.status.idle": "2022-06-06T04:17:19.286607Z",
     "shell.execute_reply": "2022-06-06T04:17:19.285720Z",
     "shell.execute_reply.started": "2022-06-06T04:17:19.281340Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- label: float (nullable = true)\n",
      " |-- hour: integer (nullable = true)\n",
      " |-- C1: integer (nullable = true)\n",
      " |-- banner_pos: integer (nullable = true)\n",
      " |-- site_id: string (nullable = true)\n",
      " |-- site_domain: string (nullable = true)\n",
      " |-- site_category: string (nullable = true)\n",
      " |-- app_id: string (nullable = true)\n",
      " |-- app_domain: string (nullable = true)\n",
      " |-- app_category: string (nullable = true)\n",
      " |-- device_id: string (nullable = true)\n",
      " |-- device_ip: string (nullable = true)\n",
      " |-- device_model: string (nullable = true)\n",
      " |-- device_type: integer (nullable = true)\n",
      " |-- device_conn_type: integer (nullable = true)\n",
      " |-- C14: integer (nullable = true)\n",
      " |-- C15: integer (nullable = true)\n",
      " |-- C16: integer (nullable = true)\n",
      " |-- C17: integer (nullable = true)\n",
      " |-- C18: integer (nullable = true)\n",
      " |-- C19: integer (nullable = true)\n",
      " |-- C20: integer (nullable = true)\n",
      " |-- C21: integer (nullable = true)\n",
      " |-- site_idIndex: double (nullable = false)\n",
      " |-- site_domainIndex: double (nullable = false)\n",
      " |-- site_categoryIndex: double (nullable = false)\n",
      " |-- app_idIndex: double (nullable = false)\n",
      " |-- app_domainIndex: double (nullable = false)\n",
      " |-- app_categoryIndex: double (nullable = false)\n",
      " |-- device_idIndex: double (nullable = false)\n",
      " |-- device_ipIndex: double (nullable = false)\n",
      " |-- device_modelIndex: double (nullable = false)\n",
      " |-- features_scaled1: vector (nullable = true)\n",
      " |-- features_scaled: vector (nullable = true)\n",
      " |-- site_id_ohe: vector (nullable = true)\n",
      " |-- site_domain_ohe: vector (nullable = true)\n",
      " |-- site_category_ohe: vector (nullable = true)\n",
      " |-- app_id_ohe: vector (nullable = true)\n",
      " |-- app_domain_ohe: vector (nullable = true)\n",
      " |-- app_category_ohe: vector (nullable = true)\n",
      " |-- device_id_ohe: vector (nullable = true)\n",
      " |-- device_ip_ohe: vector (nullable = true)\n",
      " |-- device_model_ohe: vector (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      " |-- rawPrediction: vector (nullable = true)\n",
      " |-- probability: vector (nullable = true)\n",
      " |-- prediction: double (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a90a3025-49c0-4971-9c80-b44ef342f0c5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-06T04:53:05.866697Z",
     "iopub.status.busy": "2022-06-06T04:53:05.865795Z",
     "iopub.status.idle": "2022-06-06T04:53:05.898196Z",
     "shell.execute_reply": "2022-06-06T04:53:05.897423Z",
     "shell.execute_reply.started": "2022-06-06T04:53:05.866666Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, FloatType, StringType, LongType, IntegerType, DoubleType\n",
    "from pyspark.sql.functions import split, regexp_replace\n",
    "\n",
    "pred = pred.withColumn(\"probability\", pred[\"probability\"].cast(\"String\")) \\\n",
    "    .withColumn('probabilityre', split(regexp_replace(\"probability\", \"^\\[|\\]\", \"\"), \",\")[1].cast(DoubleType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "82d3b3e0-063c-4c54-9e04-3dbc75579ed8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-06T05:05:56.747132Z",
     "iopub.status.busy": "2022-06-06T05:05:56.746194Z",
     "iopub.status.idle": "2022-06-06T05:06:09.755718Z",
     "shell.execute_reply": "2022-06-06T05:06:09.754767Z",
     "shell.execute_reply.started": "2022-06-06T05:05:56.747100Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/06/06 05:06:07 WARN DAGScheduler: Broadcasting large task binary with size 19.9 MiB\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>rawPrediction</th>\n",
       "      <th>probability</th>\n",
       "      <th>prediction</th>\n",
       "      <th>probabilityre</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9984920344968</td>\n",
       "      <td>[1.0216997729415782, -1.0216997729415782]</td>\n",
       "      <td>[0.7353035623637518,0.26469643763624817]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.264696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>441039161952503</td>\n",
       "      <td>[1.1711235751101627, -1.1711235751101627]</td>\n",
       "      <td>[0.7633480471593245,0.23665195284067553]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.236652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>488135999371288</td>\n",
       "      <td>[4.130848399311453, -4.130848399311453]</td>\n",
       "      <td>[0.9841848967820319,0.015815103217968085]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.015815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>539983806946224</td>\n",
       "      <td>[4.600912915693666, -4.600912915693666]</td>\n",
       "      <td>[0.9900571888527,0.009942811147300001]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.009943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1013507549820690</td>\n",
       "      <td>[2.1518648867926085, -2.1518648867926085]</td>\n",
       "      <td>[0.8958429149576012,0.10415708504239884]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.104157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1027051155424902</td>\n",
       "      <td>[3.027562002069021, -3.027562002069021]</td>\n",
       "      <td>[0.9538038685505382,0.046196131449461775]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.046196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1200637710450539</td>\n",
       "      <td>[0.09946731762671135, -0.09946731762671135]</td>\n",
       "      <td>[0.5248463474935845,0.4751536525064155]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.475154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1233274924027154</td>\n",
       "      <td>[6.547184902585121, -6.547184902585121]</td>\n",
       "      <td>[0.9985679066476261,0.001432093352373931]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1272541125281255</td>\n",
       "      <td>[3.9749550021280777, -3.9749550021280777]</td>\n",
       "      <td>[0.9815660458185808,0.018433954181419243]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.018434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1471506830732833</td>\n",
       "      <td>[3.9688689641686192, -3.9688689641686192]</td>\n",
       "      <td>[0.9814556008197808,0.018544399180219218]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.018544</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                rawPrediction  \\\n",
       "0     9984920344968    [1.0216997729415782, -1.0216997729415782]   \n",
       "1   441039161952503    [1.1711235751101627, -1.1711235751101627]   \n",
       "2   488135999371288      [4.130848399311453, -4.130848399311453]   \n",
       "3   539983806946224      [4.600912915693666, -4.600912915693666]   \n",
       "4  1013507549820690    [2.1518648867926085, -2.1518648867926085]   \n",
       "5  1027051155424902      [3.027562002069021, -3.027562002069021]   \n",
       "6  1200637710450539  [0.09946731762671135, -0.09946731762671135]   \n",
       "7  1233274924027154      [6.547184902585121, -6.547184902585121]   \n",
       "8  1272541125281255    [3.9749550021280777, -3.9749550021280777]   \n",
       "9  1471506830732833    [3.9688689641686192, -3.9688689641686192]   \n",
       "\n",
       "                                 probability  prediction  probabilityre  \n",
       "0   [0.7353035623637518,0.26469643763624817]         0.0       0.264696  \n",
       "1   [0.7633480471593245,0.23665195284067553]         0.0       0.236652  \n",
       "2  [0.9841848967820319,0.015815103217968085]         0.0       0.015815  \n",
       "3     [0.9900571888527,0.009942811147300001]         0.0       0.009943  \n",
       "4   [0.8958429149576012,0.10415708504239884]         0.0       0.104157  \n",
       "5  [0.9538038685505382,0.046196131449461775]         0.0       0.046196  \n",
       "6    [0.5248463474935845,0.4751536525064155]         0.0       0.475154  \n",
       "7  [0.9985679066476261,0.001432093352373931]         0.0       0.001432  \n",
       "8  [0.9815660458185808,0.018433954181419243]         0.0       0.018434  \n",
       "9  [0.9814556008197808,0.018544399180219218]         0.0       0.018544  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred.select(\"id\", \"rawPrediction\", \"probability\", \"prediction\", \"probabilityre\").limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f5a36f-e613-468d-9fbb-dd18b04fb5b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3fbcee6-1784-4964-8fff-2d3aaecefbfb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cea81b2-8f9b-4935-b289-79af4fdd23ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "915781f0-60ad-4497-bda2-e33e959b5774",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-06T04:50:19.827679Z",
     "iopub.status.busy": "2022-06-06T04:50:19.826753Z",
     "iopub.status.idle": "2022-06-06T04:50:19.877856Z",
     "shell.execute_reply": "2022-06-06T04:50:19.877049Z",
     "shell.execute_reply.started": "2022-06-06T04:50:19.827644Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorSlicer\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.sql.types import Row\n",
    "\n",
    "slicer = VectorSlicer(inputCol=\"probability\", outputCol=\"probability_\", indices=[1])\n",
    "\n",
    "pred = slicer.transform(pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf6cab8-f45a-4509-b1c9-ec086204f178",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7405bb53",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c585721c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e472df90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc5a0c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a420b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb578139",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed0d70c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ad3110",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "237d8df9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6115191c-d5cb-4279-b6f5-400f1b36337b",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_training_data.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd62740a-fda6-466b-8f79-c9cb56d1e8fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training = training.select([col for col in training.columns if col != \"id\"])\n",
    "raw_training_data.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a509ee24",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_training_data.describe().toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "950a50cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get string columns\n",
    "for name, type in raw_training_data.dtypes:\n",
    "  print(name, type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c756d2ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.ml.feature import Imputer\n",
    "\n",
    "# imputer = Imputer(\n",
    "#   inputCols=[\"Glucose\",\"BloodPressure\",\"SkinThickness\",\"BMI\",\"Insulin\"], \n",
    "#   outputCols=[\"Glucose\",\"BloodPressure\",\"SkinThickness\",\"BMI\",\"Insulin\"]\n",
    "# )\n",
    "\n",
    "# model = imputer.fit(raw_training_data)\n",
    "# raw_training_data = model.transform(raw_training_data)\n",
    "\n",
    "# raw_training_data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3c8000",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "cols = []\n",
    "pipeline_stages = []\n",
    "feature_columns = []\n",
    "\n",
    "for name, type in raw_training_data.dtypes:\n",
    "    if type == \"string\":\n",
    "        feature_columns.append(f\"{name}Index\")\n",
    "        pipeline_stages.append(StringIndexer(inputCol=name, outputCol=f\"{name}Index\"))\n",
    "    \n",
    "        cols.append(f\"{name}Index\")\n",
    "    \n",
    "    else:\n",
    "        cols.append(name)\n",
    "\n",
    "print(feature_columns)\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "raw_training_data = Pipeline(stages=pipeline_stages).fit(raw_training_data).transform(raw_training_data)\n",
    "\n",
    "raw_training_data = raw_training_data.select(cols)\n",
    "\n",
    "raw_training_data.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f19ff92",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b503ef05",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder\n",
    "\n",
    "encoder = OneHotEncoder(inputCols=feature_columns,\n",
    "                        outputCols=[col+\"_ohe\" for col in feature_columns])\n",
    "\n",
    "model = encoder.fit(raw_training_data)\n",
    "encoded = model.transform(raw_training_data)\n",
    "\n",
    "encoded.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f44d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = encoded.columns\n",
    "\n",
    "for col_to_remove in feature_columns:\n",
    "  cols.remove(col_to_remove)\n",
    "\n",
    "cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2853be9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cols=raw_training_data.columns\n",
    "cols.remove(\"id\")\n",
    "cols.remove(\"label\")\n",
    "\n",
    "# Let us import the vector assembler\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "assembler = VectorAssembler(inputCols=cols, outputCol=\"features\")\n",
    "\n",
    "# Now let us use the transform method to transform our dataset\n",
    "encoded = assembler.transform(encoded)\n",
    "encoded.select(\"features\").toPandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe27cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StandardScaler\n",
    "\n",
    "standardscaler = StandardScaler().setInputCol(\"features\").setOutputCol(\"Scaled_features\")\n",
    "encoded = standardscaler.fit(encoded).transform(encoded)\n",
    "\n",
    "encoded.select(\"features\",\"Scaled_features\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96cf42a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression(labelCol=\"label\", maxIter=10, regParam= 0.01)\n",
    "\n",
    "model=lr.fit(encoded)\n",
    "\n",
    "predict_train=model.transform(encoded)\n",
    "# predict_test=model.transform(test)\n",
    "predict_train.select(\"label\", \"prediction\", \"probability\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb02c31a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.transform(encoded).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1c8d15",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2873c16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "562fa316",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd92cdc6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6607b8df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3107fb18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.feature import OneHotEncoder\n",
    "from pyspark.ml.feature import OneHotEncoder\n",
    "from pyspark.ml.feature import MinMaxScaler\n",
    "\n",
    "# Create the logistic regression model\n",
    "lr = LogisticRegression(maxIter=10, regParam= 0.01)\n",
    "\n",
    "# We create a one hot encoder.\n",
    "# ohe = OneHotEncoder(\n",
    "#   # inputCols = [\"site_id\", \"site_domain\", \"site_category\", \"app_id\", \"app_domain\", \"app_category\", \"device_id\", \"device_ip\", \"device_model\"], \n",
    "#   inputCols = [\"site_domain\", \"site_category\", \"app_id\", \"app_domain\", \"app_category\", \"device_id\", \"device_ip\", \"device_model\"], \n",
    "#   # outputCols=[\"site_id_ohe\", \"site_domain_ohe\", \"site_category_ohe\", \"app_id_ohe\", \"app_domain_ohe\", \"app_category_ohe\", \"device_id_ohe\", \"device_ip_ohe\", \"device_model_ohe\"]\n",
    "#   outputCols=[\"site_domain_ohe\", \"site_category_ohe\", \"app_id_ohe\", \"app_domain_ohe\", \"app_category_ohe\", \"device_id_ohe\", \"device_ip_ohe\", \"device_model_ohe\"]\n",
    "# )\n",
    "\n",
    "# Input list for scaling\n",
    "inputs = [\"hour\", \"C1\", \"banner_pos\", \"device_type\", \"device_conn_type\", \"C14\", \"C15\", \"C16\", \"C17\", \"C18\", \"C19\", \"C20\", \"C21\"]\n",
    "\n",
    "# We scale our inputs\n",
    "assembler1 = VectorAssembler(inputCols=inputs, outputCol=\"features_scaled1\")\n",
    "scaler = MinMaxScaler(inputCol=\"features_scaled1\", outputCol=\"features_scaled\")\n",
    "\n",
    "# We create a second assembler for the encoded columns.\n",
    "assembler2 = VectorAssembler(\n",
    "  # inputCols=[\"site_id_ohe\", \"site_domain_ohe\", \"site_category_ohe\", \"app_id_ohe\", \"app_domain_ohe\", \"app_category_ohe\", \"device_id_ohe\", \"device_ip_ohe\", \"device_model_ohe\", 'features_scaled'], outputCol=\"features\"\n",
    "  inputCols=[\"site_domain\", \"site_category\", \"app_id\", \"app_domain\", \"app_category\", \"device_id\", \"device_ip\", \"device_model\", 'features_scaled'], outputCol=\"features\"\n",
    ")\n",
    "\n",
    "# Create stages list\n",
    "# myStages = [assembler1, scaler, ohe, assembler2,lr]\n",
    "myStages = [assembler1, scaler, assembler2, lr]\n",
    "\n",
    "# Set up the pipeline\n",
    "pipeline = Pipeline(stages= myStages)\n",
    "\n",
    "# We fit the model using the training data.\n",
    "pModel = pipeline.fit(raw_training_data)\n",
    "\n",
    "# We transform the data.\n",
    "trainingPred = pModel.transform(raw_training_data)\n",
    "\n",
    "# # We select the actual label, probability and predictions\n",
    "trainingPred.select('label', 'probability', 'prediction').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc58b80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3168238c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c995bc84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aecd2576",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "996e3382",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a62b91b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fbbe97c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f58f384b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2445a7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b55164",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807700b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d58bf8e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = raw_training_data.columns\n",
    "cols.remove(\"label\")\n",
    "\n",
    "# Let us import the vector assembler\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "assembler = VectorAssembler(inputCols=cols, outputCol=\"features\")\n",
    "\n",
    "# Now let us use the transform method to transform our dataset\n",
    "raw_training_data = assembler.transform(raw_training_data)\n",
    "raw_training_data.select(\"features\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb1505ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = raw_training_data.select(\"features\", \"label\") #.withColumnRenamed(\"click\", \"label\")\n",
    "training_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf6b58f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression(maxIter=10, regParam=0.3, elasticNetParam=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b79ef0a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = lr.fit(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c29fab7c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f344cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = spark.read.format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .load('../dataset/click-through-rate-prediction/test.gz')\n",
    "\n",
    "test.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa585a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ba9425",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, FloatType, StringType, LongType, IntegerType\n",
    "\n",
    "schema = StructType([\n",
    "    StructField('id', LongType(), True),\n",
    "    StructField(\"hour\", IntegerType(), True),\n",
    "    StructField(\"C1\", IntegerType(), True),\n",
    "    StructField(\"banner_pos\", IntegerType(), True),\n",
    "    StructField(\"site_id\", StringType(), True),\n",
    "    StructField(\"site_domain\", StringType(), True),\n",
    "    StructField(\"site_category\", StringType(), True),\n",
    "    StructField(\"app_id\", StringType(), True),\n",
    "    StructField(\"app_domain\", StringType(), True),\n",
    "    StructField(\"app_category\", StringType(), True),\n",
    "    StructField(\"device_id\", StringType(), True),\n",
    "    StructField(\"device_ip\", StringType(), True),\n",
    "    StructField(\"device_model\", StringType(), True),\n",
    "    StructField(\"device_type\", IntegerType(), True),\n",
    "    StructField(\"device_conn_type\", IntegerType(), True),\n",
    "    StructField(\"C14\", IntegerType(), True),\n",
    "    StructField(\"C15\", IntegerType(), True),\n",
    "    StructField(\"C16\", IntegerType(), True),\n",
    "    StructField(\"C17\", IntegerType(), True),\n",
    "    StructField(\"C18\", IntegerType(), True),\n",
    "    StructField(\"C19\", IntegerType(), True),\n",
    "    StructField(\"C20\", IntegerType(), True),\n",
    "    StructField(\"C21\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "# Load test data\n",
    "test = spark.read.format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .schema(schema) \\\n",
    "    .load('../dataset/click-through-rate-prediction/test.gz')\n",
    "\n",
    "test = test.withColumnRenamed(\"click\", \"label\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be9f85b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = pModel.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a906ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bbb2cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "\n",
    "accuracy = evaluator.evaluate(pred)\n",
    "\n",
    "print(\"Train Accuracy = %g \" % (accuracy))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3aa3ae8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b72e8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fe3cb5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7772c1da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "cols = []\n",
    "pipeline_stages = []\n",
    "feature_columns = []\n",
    "\n",
    "for name, type in test.dtypes:\n",
    "    if type == \"string\":\n",
    "        feature_columns.append(f\"{name}Index\")\n",
    "        pipeline_stages.append(StringIndexer(inputCol=name, outputCol=f\"{name}Index\"))\n",
    "    \n",
    "        cols.append(f\"{name}Index\")\n",
    "    \n",
    "    else:\n",
    "        cols.append(name)\n",
    "\n",
    "cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde70f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae9f670",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "\n",
    "test = Pipeline(stages=pipeline_stages).fit(test).transform(test)\n",
    "\n",
    "test = test.select(cols)\n",
    "\n",
    "test.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d625e5b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = test.columns\n",
    "# cols.remove(\"click\")\n",
    "\n",
    "# Let us import the vector assembler\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "assembler = VectorAssembler(inputCols=cols, outputCol=\"features\")\n",
    "\n",
    "# Now let us use the transform method to transform our dataset\n",
    "test = assembler.transform(test)\n",
    "test = test.select(\"features\")\n",
    "\n",
    "test.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb12eb59",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc8be501",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5137f3a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae2f827",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StandardScaler\n",
    "\n",
    "standardscaler = StandardScaler().setInputCol(\"features\").setOutputCol(\"Scaled_features\")\n",
    "raw_training_data = standardscaler.fit(raw_training_data).transform(raw_training_data)\n",
    "raw_training_data.select(\"features\",\"Scaled_features\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "652a895e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b39e881b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fbd0437",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc282ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d13dfa0-61a3-4d6c-93f0-60d1c35b6704",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed9417e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import HashingTF, Tokenizer, VectorAssembler\n",
    "\n",
    "# Creating Vector Assembler\n",
    "vecAssembler = VectorAssembler(inputCols=[col if type != \"string\" else col+\"Index\" for col, type in training.dtypes if col != \"id\"], outputCol=\"features\")\n",
    "pipeline_stages.append(vecAssembler)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0048659c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# Creating Logistic Regression Model\n",
    "lr = LogisticRegression(maxIter=10, regParam=0.3, elasticNetParam=0.8)\n",
    "pipeline_stages.append(lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f9ef2de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# Creating pipeline\n",
    "pipeline = Pipeline(stages=pipeline_stages)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152af103",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Executing pipeline (VectorAssembler + LogisticRegression)\n",
    "lrModel = pipeline.fit(training)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed50f865",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "646a395b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa23e60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e65936",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e19dcc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00134877",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6908e48f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e319d2a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b39ba777",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Configure an ML pipeline, which consists of three stages: tokenizer, hashingTF, and lr.\n",
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "hashingTF = HashingTF(inputCol=tokenizer.getOutputCol(), outputCol=\"features\")\n",
    "lr = LogisticRegression(maxIter=10, regParam=0.001)\n",
    "\n",
    "pipeline = Pipeline(stages=[tokenizer, hashingTF, lr])\n",
    "\n",
    "# Fit the pipeline to training documents.\n",
    "model = pipeline.fit(training)\n",
    "\n",
    "# Prepare test documents, which are unlabeled (id, text) tuples.\n",
    "test = spark.createDataFrame([\n",
    "    (4, \"spark i j k\"),\n",
    "    (5, \"l m n\"),\n",
    "    (6, \"spark hadoop spark\"),\n",
    "    (7, \"apache hadoop\")\n",
    "], [\"id\", \"text\"])\n",
    "\n",
    "# Make predictions on test documents and print columns of interest.\n",
    "prediction = model.transform(test)\n",
    "selected = prediction.select(\"id\", \"text\", \"probability\", \"prediction\")\n",
    "for row in selected.collect():\n",
    "    rid, text, prob, prediction = row  # type: ignore\n",
    "    print(\n",
    "        \"(%d, %s) --> prob=%s, prediction=%f\" % (\n",
    "            rid, text, str(prob), prediction   # type: ignore\n",
    "        )\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8457bb32",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression(maxIter=10, regParam=0.3, elasticNetParam=0.8)\n",
    "\n",
    "# Fit the model\n",
    "lrModel = lr.fit(training)\n",
    "\n",
    "# Print the coefficients and intercept for logistic regression\n",
    "print(\"Coefficients: \" + str(lrModel.coefficients))\n",
    "print(\"Intercept: \" + str(lrModel.intercept))\n",
    "\n",
    "# We can also use the multinomial family for binary classification\n",
    "mlr = LogisticRegression(maxIter=10, regParam=0.3, elasticNetParam=0.8, family=\"multinomial\")\n",
    "\n",
    "# Fit the model\n",
    "mlrModel = mlr.fit(training)\n",
    "\n",
    "# Print the coefficients and intercepts for logistic regression with multinomial family\n",
    "print(\"Multinomial coefficients: \" + str(mlrModel.coefficientMatrix))\n",
    "print(\"Multinomial intercepts: \" + str(mlrModel.interceptVector))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae51eef9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "23290619d6f4f5ecea54fd3814d63b8a6a2d2c019f5870989b08cbcfb848aa36"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
