{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d10186c7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-02T17:03:27.347518Z",
     "iopub.status.busy": "2022-06-02T17:03:27.343736Z",
     "iopub.status.idle": "2022-06-02T17:03:44.288697Z",
     "shell.execute_reply": "2022-06-02T17:03:44.288095Z",
     "shell.execute_reply.started": "2022-06-02T17:03:27.347176Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# import pyspark.pandas as ps\n",
    "import pandas as pd\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .master('local[*]') \\\n",
    "    .config(\"spark.driver.memory\", \"24g\") \\\n",
    "    .config(\"spark.kryoserializer.buffer.max\", \"512m\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# .config(\"spark.executor.memory\", \"8g\") \\\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19bcdcbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, FloatType, StringType, LongType, IntegerType\n",
    "\n",
    "schema = StructType([\n",
    "    StructField('id', LongType(), True),\n",
    "    StructField(\"click\", FloatType(), True),\n",
    "    StructField(\"hour\", IntegerType(), True),\n",
    "    StructField(\"C1\", IntegerType(), True),\n",
    "    StructField(\"banner_pos\", IntegerType(), True),\n",
    "    StructField(\"site_id\", StringType(), True),\n",
    "    StructField(\"site_domain\", StringType(), True),\n",
    "    StructField(\"site_category\", StringType(), True),\n",
    "    StructField(\"app_id\", StringType(), True),\n",
    "    StructField(\"app_domain\", StringType(), True),\n",
    "    StructField(\"app_category\", StringType(), True),\n",
    "    StructField(\"device_id\", StringType(), True),\n",
    "    StructField(\"device_ip\", StringType(), True),\n",
    "    StructField(\"device_model\", StringType(), True),\n",
    "    StructField(\"device_type\", IntegerType(), True),\n",
    "    StructField(\"device_conn_type\", IntegerType(), True),\n",
    "    StructField(\"C14\", IntegerType(), True),\n",
    "    StructField(\"C15\", IntegerType(), True),\n",
    "    StructField(\"C16\", IntegerType(), True),\n",
    "    StructField(\"C17\", IntegerType(), True),\n",
    "    StructField(\"C18\", IntegerType(), True),\n",
    "    StructField(\"C19\", IntegerType(), True),\n",
    "    StructField(\"C20\", IntegerType(), True),\n",
    "    StructField(\"C21\", IntegerType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a717f726",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-02T17:03:44.290361Z",
     "iopub.status.busy": "2022-06-02T17:03:44.289965Z",
     "iopub.status.idle": "2022-06-02T17:07:36.173214Z",
     "shell.execute_reply": "2022-06-02T17:07:36.172262Z",
     "shell.execute_reply.started": "2022-06-02T17:03:44.290337Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load training data\n",
    "# training = spark.read.format(\"csv\") \\\n",
    "#     .option(\"header\", \"true\") \\\n",
    "#     .option(\"inferSchema\", \"true\") \\\n",
    "#     .load('../dataset/click-through-rate-prediction/train.gz')\n",
    "\n",
    "training = spark.read.format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .schema(schema) \\\n",
    "    .load('../dataset/click-through-rate-prediction/train.gz')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f74496ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_training_data = training.limit(1000)\n",
    "\n",
    "raw_training_data = raw_training_data.na.drop() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d58ddb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-02T17:07:36.175913Z",
     "iopub.status.busy": "2022-06-02T17:07:36.175465Z",
     "iopub.status.idle": "2022-06-02T17:07:36.189459Z",
     "shell.execute_reply": "2022-06-02T17:07:36.188841Z",
     "shell.execute_reply.started": "2022-06-02T17:07:36.175875Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "raw_training_data = raw_training_data.withColumnRenamed(\"click\", \"label\")\n",
    "\n",
    "raw_training_data.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6115191c-d5cb-4279-b6f5-400f1b36337b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-02T17:07:36.192715Z",
     "iopub.status.busy": "2022-06-02T17:07:36.192374Z",
     "iopub.status.idle": "2022-06-02T17:07:36.237606Z",
     "shell.execute_reply": "2022-06-02T17:07:36.234751Z",
     "shell.execute_reply.started": "2022-06-02T17:07:36.192693Z"
    }
   },
   "outputs": [],
   "source": [
    "raw_training_data.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd62740a-fda6-466b-8f79-c9cb56d1e8fa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-02T17:07:37.222278Z",
     "iopub.status.busy": "2022-06-02T17:07:37.221721Z",
     "iopub.status.idle": "2022-06-02T17:07:37.659019Z",
     "shell.execute_reply": "2022-06-02T17:07:37.658093Z",
     "shell.execute_reply.started": "2022-06-02T17:07:37.222236Z"
    }
   },
   "outputs": [],
   "source": [
    "# training = training.select([col for col in training.columns if col != \"id\"])\n",
    "raw_training_data.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a509ee24",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_training_data.describe().toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "950a50cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get string columns\n",
    "for name, type in raw_training_data.dtypes:\n",
    "  print(name, type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c756d2ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.ml.feature import Imputer\n",
    "\n",
    "# imputer = Imputer(\n",
    "#   inputCols=[\"Glucose\",\"BloodPressure\",\"SkinThickness\",\"BMI\",\"Insulin\"], \n",
    "#   outputCols=[\"Glucose\",\"BloodPressure\",\"SkinThickness\",\"BMI\",\"Insulin\"]\n",
    "# )\n",
    "\n",
    "# model = imputer.fit(raw_training_data)\n",
    "# raw_training_data = model.transform(raw_training_data)\n",
    "\n",
    "# raw_training_data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3c8000",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "cols = []\n",
    "pipeline_stages = []\n",
    "feature_columns = []\n",
    "\n",
    "for name, type in raw_training_data.dtypes:\n",
    "    if type == \"string\":\n",
    "        feature_columns.append(f\"{name}Index\")\n",
    "        pipeline_stages.append(StringIndexer(inputCol=name, outputCol=f\"{name}Index\"))\n",
    "    \n",
    "        cols.append(f\"{name}Index\")\n",
    "    \n",
    "    else:\n",
    "        cols.append(name)\n",
    "\n",
    "print(feature_columns)\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "raw_training_data = Pipeline(stages=pipeline_stages).fit(raw_training_data).transform(raw_training_data)\n",
    "\n",
    "raw_training_data = raw_training_data.select(cols)\n",
    "\n",
    "raw_training_data.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f19ff92",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b503ef05",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder\n",
    "\n",
    "encoder = OneHotEncoder(inputCols=feature_columns,\n",
    "                        outputCols=[col+\"_ohe\" for col in feature_columns])\n",
    "\n",
    "model = encoder.fit(raw_training_data)\n",
    "encoded = model.transform(raw_training_data)\n",
    "\n",
    "encoded.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f44d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = encoded.columns\n",
    "\n",
    "for col_to_remove in feature_columns:\n",
    "  cols.remove(col_to_remove)\n",
    "\n",
    "cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2853be9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cols=raw_training_data.columns\n",
    "cols.remove(\"id\")\n",
    "cols.remove(\"label\")\n",
    "\n",
    "# Let us import the vector assembler\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "assembler = VectorAssembler(inputCols=cols, outputCol=\"features\")\n",
    "\n",
    "# Now let us use the transform method to transform our dataset\n",
    "encoded = assembler.transform(encoded)\n",
    "encoded.select(\"features\").toPandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe27cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StandardScaler\n",
    "\n",
    "standardscaler = StandardScaler().setInputCol(\"features\").setOutputCol(\"Scaled_features\")\n",
    "encoded = standardscaler.fit(encoded).transform(encoded)\n",
    "\n",
    "encoded.select(\"features\",\"Scaled_features\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96cf42a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression(labelCol=\"label\", maxIter=10, regParam= 0.01)\n",
    "\n",
    "model=lr.fit(encoded)\n",
    "\n",
    "predict_train=model.transform(encoded)\n",
    "# predict_test=model.transform(test)\n",
    "predict_train.select(\"label\", \"prediction\", \"probability\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb02c31a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.transform(encoded).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed0d70c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ad3110",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "237d8df9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1c8d15",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2873c16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "c3b1ff7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_training_data = training.limit(1000)\n",
    "\n",
    "raw_training_data = raw_training_data.withColumnRenamed(\"click\", \"label\")\n",
    "\n",
    "raw_training_data = raw_training_data.na.drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "688e00a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the logistic regression model\n",
    "lr = LogisticRegression(maxIter=10, regParam= 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "679a90f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a StringIndexer\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "cols = []\n",
    "pipeline_stages = []\n",
    "feature_columns = []\n",
    "\n",
    "for name, type in raw_training_data.dtypes:\n",
    "    if type == \"string\":\n",
    "        feature_columns.append(f\"{name}Index\")\n",
    "        pipeline_stages.append(StringIndexer(inputCol=name, outputCol=f\"{name}Index\"))\n",
    "    \n",
    "        cols.append(f\"{name}Index\")\n",
    "    \n",
    "    else:\n",
    "        cols.append(name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "680047ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a one hot encoder\n",
    "feature_columns = ['site_idIndex', 'site_domainIndex', 'site_categoryIndex', 'app_idIndex', 'app_domainIndex', 'app_categoryIndex', 'device_idIndex', 'device_ipIndex', 'device_modelIndex']\n",
    "output_ohe_columns = ['site_id_ohe', 'site_domain_ohe', 'site_category_ohe', 'app_id_ohe', 'app_domain_ohe', 'app_category_ohe', 'device_id_ohe', 'device_ip_ohe', 'device_model_ohe']\n",
    "\n",
    "ohe = OneHotEncoder(inputCols = feature_columns, outputCols = output_ohe_columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "1bcaedc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import MinMaxScaler\n",
    "\n",
    "# Input list for scaling\n",
    "inputs = [\"hour\", \"C1\", \"banner_pos\", \"device_type\", \"device_conn_type\", \"C14\", \"C15\", \"C16\", \"C17\", \"C18\", \"C19\", \"C20\", \"C21\"]\n",
    "\n",
    "# We scale our inputs\n",
    "assembler1 = VectorAssembler(inputCols=inputs, outputCol=\"features_scaled1\")\n",
    "scaler = MinMaxScaler(inputCol=\"features_scaled1\", outputCol=\"features_scaled\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "6ab4af82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create a second assembler for the encoded columns.\n",
    "assembler2 = VectorAssembler(\n",
    "  inputCols=['features_scaled'] + output_ohe_columns, outputCol=\"features\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "fe0fe8e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+----------+\n",
      "|label|         probability|prediction|\n",
      "+-----+--------------------+----------+\n",
      "|  0.0|[0.99448176328146...|       0.0|\n",
      "|  0.0|[0.99802823200410...|       0.0|\n",
      "|  0.0|[0.99295653865260...|       0.0|\n",
      "|  1.0|[0.03449483439795...|       1.0|\n",
      "|  0.0|[0.99386086643276...|       0.0|\n",
      "|  0.0|[0.99795437760237...|       0.0|\n",
      "|  0.0|[0.99295658072748...|       0.0|\n",
      "|  0.0|[0.99880661696309...|       0.0|\n",
      "|  0.0|[0.99457899494175...|       0.0|\n",
      "|  0.0|[0.99157812715337...|       0.0|\n",
      "|  1.0|[0.06347798172384...|       1.0|\n",
      "|  0.0|[0.99814316731984...|       0.0|\n",
      "|  0.0|[0.99211318819529...|       0.0|\n",
      "|  1.0|[0.02023344102929...|       1.0|\n",
      "|  0.0|[0.99804042434207...|       0.0|\n",
      "|  0.0|[0.99448183108153...|       0.0|\n",
      "|  0.0|[0.99457899494175...|       0.0|\n",
      "|  0.0|[0.98886697514116...|       0.0|\n",
      "|  0.0|[0.99208547417497...|       0.0|\n",
      "|  0.0|[0.99814316731984...|       0.0|\n",
      "+-----+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create stages list\n",
    "myStages = pipeline_stages + [assembler1, scaler, ohe, assembler2, lr]\n",
    "\n",
    "# Set up the pipeline\n",
    "pipeline = Pipeline(stages= myStages)\n",
    "\n",
    "# We fit the model using the training data.\n",
    "pModel = pipeline.fit(raw_training_data)\n",
    "\n",
    "# We transform the data.\n",
    "trainingPred = pModel.transform(raw_training_data)\n",
    "\n",
    "# # We select the actual label, probability and predictions\n",
    "trainingPred.select('label', 'probability', 'prediction').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3da2151",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "635bc295",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "461169c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "562fa316",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd92cdc6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6607b8df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3107fb18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.feature import OneHotEncoder\n",
    "from pyspark.ml.feature import OneHotEncoder\n",
    "from pyspark.ml.feature import MinMaxScaler\n",
    "\n",
    "# Create the logistic regression model\n",
    "lr = LogisticRegression(maxIter=10, regParam= 0.01)\n",
    "\n",
    "# We create a one hot encoder.\n",
    "# ohe = OneHotEncoder(\n",
    "#   # inputCols = [\"site_id\", \"site_domain\", \"site_category\", \"app_id\", \"app_domain\", \"app_category\", \"device_id\", \"device_ip\", \"device_model\"], \n",
    "#   inputCols = [\"site_domain\", \"site_category\", \"app_id\", \"app_domain\", \"app_category\", \"device_id\", \"device_ip\", \"device_model\"], \n",
    "#   # outputCols=[\"site_id_ohe\", \"site_domain_ohe\", \"site_category_ohe\", \"app_id_ohe\", \"app_domain_ohe\", \"app_category_ohe\", \"device_id_ohe\", \"device_ip_ohe\", \"device_model_ohe\"]\n",
    "#   outputCols=[\"site_domain_ohe\", \"site_category_ohe\", \"app_id_ohe\", \"app_domain_ohe\", \"app_category_ohe\", \"device_id_ohe\", \"device_ip_ohe\", \"device_model_ohe\"]\n",
    "# )\n",
    "\n",
    "# Input list for scaling\n",
    "inputs = [\"hour\", \"C1\", \"banner_pos\", \"device_type\", \"device_conn_type\", \"C14\", \"C15\", \"C16\", \"C17\", \"C18\", \"C19\", \"C20\", \"C21\"]\n",
    "\n",
    "# We scale our inputs\n",
    "assembler1 = VectorAssembler(inputCols=inputs, outputCol=\"features_scaled1\")\n",
    "scaler = MinMaxScaler(inputCol=\"features_scaled1\", outputCol=\"features_scaled\")\n",
    "\n",
    "# We create a second assembler for the encoded columns.\n",
    "assembler2 = VectorAssembler(\n",
    "  # inputCols=[\"site_id_ohe\", \"site_domain_ohe\", \"site_category_ohe\", \"app_id_ohe\", \"app_domain_ohe\", \"app_category_ohe\", \"device_id_ohe\", \"device_ip_ohe\", \"device_model_ohe\", 'features_scaled'], outputCol=\"features\"\n",
    "  inputCols=[\"site_domain\", \"site_category\", \"app_id\", \"app_domain\", \"app_category\", \"device_id\", \"device_ip\", \"device_model\", 'features_scaled'], outputCol=\"features\"\n",
    ")\n",
    "\n",
    "# Create stages list\n",
    "# myStages = [assembler1, scaler, ohe, assembler2,lr]\n",
    "myStages = [assembler1, scaler, assembler2, lr]\n",
    "\n",
    "# Set up the pipeline\n",
    "pipeline = Pipeline(stages= myStages)\n",
    "\n",
    "# We fit the model using the training data.\n",
    "pModel = pipeline.fit(raw_training_data)\n",
    "\n",
    "# We transform the data.\n",
    "trainingPred = pModel.transform(raw_training_data)\n",
    "\n",
    "# # We select the actual label, probability and predictions\n",
    "trainingPred.select('label', 'probability', 'prediction').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc58b80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3168238c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c995bc84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aecd2576",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "996e3382",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a62b91b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fbbe97c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f58f384b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2445a7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b55164",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-02T17:07:37.660549Z",
     "iopub.status.busy": "2022-06-02T17:07:37.660168Z",
     "iopub.status.idle": "2022-06-02T17:07:37.888200Z",
     "shell.execute_reply": "2022-06-02T17:07:37.887069Z",
     "shell.execute_reply.started": "2022-06-02T17:07:37.660516Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807700b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d58bf8e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = raw_training_data.columns\n",
    "cols.remove(\"label\")\n",
    "\n",
    "# Let us import the vector assembler\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "assembler = VectorAssembler(inputCols=cols, outputCol=\"features\")\n",
    "\n",
    "# Now let us use the transform method to transform our dataset\n",
    "raw_training_data = assembler.transform(raw_training_data)\n",
    "raw_training_data.select(\"features\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb1505ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = raw_training_data.select(\"features\", \"label\") #.withColumnRenamed(\"click\", \"label\")\n",
    "training_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf6b58f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression(maxIter=10, regParam=0.3, elasticNetParam=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b79ef0a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = lr.fit(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c29fab7c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f344cb5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa585a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ba9425",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, FloatType, StringType, LongType, IntegerType\n",
    "\n",
    "schema = StructType([\n",
    "    StructField('id', LongType(), True),\n",
    "    StructField(\"hour\", IntegerType(), True),\n",
    "    StructField(\"C1\", IntegerType(), True),\n",
    "    StructField(\"banner_pos\", IntegerType(), True),\n",
    "    StructField(\"site_id\", StringType(), True),\n",
    "    StructField(\"site_domain\", StringType(), True),\n",
    "    StructField(\"site_category\", StringType(), True),\n",
    "    StructField(\"app_id\", StringType(), True),\n",
    "    StructField(\"app_domain\", StringType(), True),\n",
    "    StructField(\"app_category\", StringType(), True),\n",
    "    StructField(\"device_id\", StringType(), True),\n",
    "    StructField(\"device_ip\", StringType(), True),\n",
    "    StructField(\"device_model\", StringType(), True),\n",
    "    StructField(\"device_type\", IntegerType(), True),\n",
    "    StructField(\"device_conn_type\", IntegerType(), True),\n",
    "    StructField(\"C14\", IntegerType(), True),\n",
    "    StructField(\"C15\", IntegerType(), True),\n",
    "    StructField(\"C16\", IntegerType(), True),\n",
    "    StructField(\"C17\", IntegerType(), True),\n",
    "    StructField(\"C18\", IntegerType(), True),\n",
    "    StructField(\"C19\", IntegerType(), True),\n",
    "    StructField(\"C20\", IntegerType(), True),\n",
    "    StructField(\"C21\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "# Load test data\n",
    "test = spark.read.format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .schema(schema) \\\n",
    "    .load('../dataset/click-through-rate-prediction/test.gz')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7772c1da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "cols = []\n",
    "pipeline_stages = []\n",
    "feature_columns = []\n",
    "\n",
    "for name, type in test.dtypes:\n",
    "    if type == \"string\":\n",
    "        feature_columns.append(f\"{name}Index\")\n",
    "        pipeline_stages.append(StringIndexer(inputCol=name, outputCol=f\"{name}Index\"))\n",
    "    \n",
    "        cols.append(f\"{name}Index\")\n",
    "    \n",
    "    else:\n",
    "        cols.append(name)\n",
    "\n",
    "cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde70f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae9f670",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "\n",
    "test = Pipeline(stages=pipeline_stages).fit(test).transform(test)\n",
    "\n",
    "test = test.select(cols)\n",
    "\n",
    "test.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d625e5b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = test.columns\n",
    "# cols.remove(\"click\")\n",
    "\n",
    "# Let us import the vector assembler\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "assembler = VectorAssembler(inputCols=cols, outputCol=\"features\")\n",
    "\n",
    "# Now let us use the transform method to transform our dataset\n",
    "test = assembler.transform(test)\n",
    "test = test.select(\"features\")\n",
    "\n",
    "test.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb12eb59",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc8be501",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5137f3a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae2f827",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StandardScaler\n",
    "\n",
    "standardscaler = StandardScaler().setInputCol(\"features\").setOutputCol(\"Scaled_features\")\n",
    "raw_training_data = standardscaler.fit(raw_training_data).transform(raw_training_data)\n",
    "raw_training_data.select(\"features\",\"Scaled_features\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "652a895e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b39e881b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fbd0437",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc282ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d13dfa0-61a3-4d6c-93f0-60d1c35b6704",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-02T17:07:37.891601Z",
     "iopub.status.busy": "2022-06-02T17:07:37.889907Z",
     "iopub.status.idle": "2022-06-02T17:07:37.903945Z",
     "shell.execute_reply": "2022-06-02T17:07:37.899302Z",
     "shell.execute_reply.started": "2022-06-02T17:07:37.891544Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed9417e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-02T17:07:37.906020Z",
     "iopub.status.busy": "2022-06-02T17:07:37.905364Z",
     "iopub.status.idle": "2022-06-02T17:07:37.930800Z",
     "shell.execute_reply": "2022-06-02T17:07:37.930219Z",
     "shell.execute_reply.started": "2022-06-02T17:07:37.905972Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import HashingTF, Tokenizer, VectorAssembler\n",
    "\n",
    "# Creating Vector Assembler\n",
    "vecAssembler = VectorAssembler(inputCols=[col if type != \"string\" else col+\"Index\" for col, type in training.dtypes if col != \"id\"], outputCol=\"features\")\n",
    "pipeline_stages.append(vecAssembler)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0048659c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-02T17:07:37.932265Z",
     "iopub.status.busy": "2022-06-02T17:07:37.932059Z",
     "iopub.status.idle": "2022-06-02T17:07:38.054400Z",
     "shell.execute_reply": "2022-06-02T17:07:38.053236Z",
     "shell.execute_reply.started": "2022-06-02T17:07:37.932249Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# Creating Logistic Regression Model\n",
    "lr = LogisticRegression(maxIter=10, regParam=0.3, elasticNetParam=0.8)\n",
    "pipeline_stages.append(lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f9ef2de",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-02T17:07:38.057369Z",
     "iopub.status.busy": "2022-06-02T17:07:38.055375Z",
     "iopub.status.idle": "2022-06-02T17:07:38.076242Z",
     "shell.execute_reply": "2022-06-02T17:07:38.060556Z",
     "shell.execute_reply.started": "2022-06-02T17:07:38.057334Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# Creating pipeline\n",
    "pipeline = Pipeline(stages=pipeline_stages)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152af103",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-02T17:07:38.078057Z",
     "iopub.status.busy": "2022-06-02T17:07:38.077615Z",
     "iopub.status.idle": "2022-06-02T17:22:17.399769Z",
     "shell.execute_reply": "2022-06-02T17:22:17.398751Z",
     "shell.execute_reply.started": "2022-06-02T17:07:38.078022Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Executing pipeline (VectorAssembler + LogisticRegression)\n",
    "lrModel = pipeline.fit(training)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed50f865",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "646a395b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa23e60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e65936",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e19dcc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00134877",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6908e48f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e319d2a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b39ba777",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Configure an ML pipeline, which consists of three stages: tokenizer, hashingTF, and lr.\n",
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "hashingTF = HashingTF(inputCol=tokenizer.getOutputCol(), outputCol=\"features\")\n",
    "lr = LogisticRegression(maxIter=10, regParam=0.001)\n",
    "\n",
    "pipeline = Pipeline(stages=[tokenizer, hashingTF, lr])\n",
    "\n",
    "# Fit the pipeline to training documents.\n",
    "model = pipeline.fit(training)\n",
    "\n",
    "# Prepare test documents, which are unlabeled (id, text) tuples.\n",
    "test = spark.createDataFrame([\n",
    "    (4, \"spark i j k\"),\n",
    "    (5, \"l m n\"),\n",
    "    (6, \"spark hadoop spark\"),\n",
    "    (7, \"apache hadoop\")\n",
    "], [\"id\", \"text\"])\n",
    "\n",
    "# Make predictions on test documents and print columns of interest.\n",
    "prediction = model.transform(test)\n",
    "selected = prediction.select(\"id\", \"text\", \"probability\", \"prediction\")\n",
    "for row in selected.collect():\n",
    "    rid, text, prob, prediction = row  # type: ignore\n",
    "    print(\n",
    "        \"(%d, %s) --> prob=%s, prediction=%f\" % (\n",
    "            rid, text, str(prob), prediction   # type: ignore\n",
    "        )\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8457bb32",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression(maxIter=10, regParam=0.3, elasticNetParam=0.8)\n",
    "\n",
    "# Fit the model\n",
    "lrModel = lr.fit(training)\n",
    "\n",
    "# Print the coefficients and intercept for logistic regression\n",
    "print(\"Coefficients: \" + str(lrModel.coefficients))\n",
    "print(\"Intercept: \" + str(lrModel.intercept))\n",
    "\n",
    "# We can also use the multinomial family for binary classification\n",
    "mlr = LogisticRegression(maxIter=10, regParam=0.3, elasticNetParam=0.8, family=\"multinomial\")\n",
    "\n",
    "# Fit the model\n",
    "mlrModel = mlr.fit(training)\n",
    "\n",
    "# Print the coefficients and intercepts for logistic regression with multinomial family\n",
    "print(\"Multinomial coefficients: \" + str(mlrModel.coefficientMatrix))\n",
    "print(\"Multinomial intercepts: \" + str(mlrModel.interceptVector))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae51eef9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "23290619d6f4f5ecea54fd3814d63b8a6a2d2c019f5870989b08cbcfb848aa36"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 ('spark-ml-training-Z1-rjbZ7-py3.9')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
