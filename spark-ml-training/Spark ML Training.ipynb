{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d10186c7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-02T17:03:27.347518Z",
     "iopub.status.busy": "2022-06-02T17:03:27.343736Z",
     "iopub.status.idle": "2022-06-02T17:03:44.288697Z",
     "shell.execute_reply": "2022-06-02T17:03:44.288095Z",
     "shell.execute_reply.started": "2022-06-02T17:03:27.347176Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/06/03 21:18:38 WARN Utils: Your hostname, carloshkayser resolves to a loopback address: 127.0.1.1; using 10.32.45.215 instead (on interface ens160)\n",
      "22/06/03 21:18:38 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/kayser/.cache/pypoetry/virtualenvs/spark-ml-training-Z1-rjbZ7-py3.9/lib/python3.9/site-packages/pyspark/jars/spark-unsafe_2.12-3.2.0.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/06/03 21:18:38 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.32.45.215:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>spark://carloshkayser:7077</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Spark ML</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f1edf67aca0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# import pyspark.pandas as ps\n",
    "import pandas as pd\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "# .master('local[*]') \\\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Spark ML\") \\\n",
    "    .master(\"spark://carloshkayser:7077\") \\\n",
    "    .getOrCreate()\n",
    "# .config(\"spark.driver.memory\", \"24g\") \\\n",
    "# .config(\"spark.kryoserializer.buffer.max\", \"512m\") \\\n",
    "\n",
    "# .config(\"spark.executor.memory\", \"8g\") \\\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "19bcdcbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, FloatType, StringType, LongType, IntegerType\n",
    "\n",
    "schema = StructType([\n",
    "    StructField('id', LongType(), True),\n",
    "    StructField(\"click\", FloatType(), True),\n",
    "    StructField(\"hour\", IntegerType(), True),\n",
    "    StructField(\"C1\", IntegerType(), True),\n",
    "    StructField(\"banner_pos\", IntegerType(), True),\n",
    "    StructField(\"site_id\", StringType(), True),\n",
    "    StructField(\"site_domain\", StringType(), True),\n",
    "    StructField(\"site_category\", StringType(), True),\n",
    "    StructField(\"app_id\", StringType(), True),\n",
    "    StructField(\"app_domain\", StringType(), True),\n",
    "    StructField(\"app_category\", StringType(), True),\n",
    "    StructField(\"device_id\", StringType(), True),\n",
    "    StructField(\"device_ip\", StringType(), True),\n",
    "    StructField(\"device_model\", StringType(), True),\n",
    "    StructField(\"device_type\", IntegerType(), True),\n",
    "    StructField(\"device_conn_type\", IntegerType(), True),\n",
    "    StructField(\"C14\", IntegerType(), True),\n",
    "    StructField(\"C15\", IntegerType(), True),\n",
    "    StructField(\"C16\", IntegerType(), True),\n",
    "    StructField(\"C17\", IntegerType(), True),\n",
    "    StructField(\"C18\", IntegerType(), True),\n",
    "    StructField(\"C19\", IntegerType(), True),\n",
    "    StructField(\"C20\", IntegerType(), True),\n",
    "    StructField(\"C21\", IntegerType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a717f726",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-02T17:03:44.290361Z",
     "iopub.status.busy": "2022-06-02T17:03:44.289965Z",
     "iopub.status.idle": "2022-06-02T17:07:36.173214Z",
     "shell.execute_reply": "2022-06-02T17:07:36.172262Z",
     "shell.execute_reply.started": "2022-06-02T17:03:44.290337Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load training data\n",
    "# training = spark.read.format(\"csv\") \\\n",
    "#     .option(\"header\", \"true\") \\\n",
    "#     .option(\"inferSchema\", \"true\") \\\n",
    "#     .load('../dataset/click-through-rate-prediction/train.gz')\n",
    "\n",
    "df = spark.read.format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .schema(schema) \\\n",
    "    .load('../dataset/click-through-rate-prediction/train.gz')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2cdce35e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.na.drop() \n",
    "\n",
    "df = df.withColumnRenamed(\"click\", \"label\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "490a3395",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data, raw_training_data = df.randomSplit([0.3, 0.7])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f74496ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw_training_data = training.limit(1000)\n",
    "\n",
    "# raw_training_data = raw_training_data.na.drop() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c0d58ddb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-02T17:07:36.175913Z",
     "iopub.status.busy": "2022-06-02T17:07:36.175465Z",
     "iopub.status.idle": "2022-06-02T17:07:36.189459Z",
     "shell.execute_reply": "2022-06-02T17:07:36.188841Z",
     "shell.execute_reply.started": "2022-06-02T17:07:36.175875Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- label: float (nullable = true)\n",
      " |-- hour: integer (nullable = true)\n",
      " |-- C1: integer (nullable = true)\n",
      " |-- banner_pos: integer (nullable = true)\n",
      " |-- site_id: string (nullable = true)\n",
      " |-- site_domain: string (nullable = true)\n",
      " |-- site_category: string (nullable = true)\n",
      " |-- app_id: string (nullable = true)\n",
      " |-- app_domain: string (nullable = true)\n",
      " |-- app_category: string (nullable = true)\n",
      " |-- device_id: string (nullable = true)\n",
      " |-- device_ip: string (nullable = true)\n",
      " |-- device_model: string (nullable = true)\n",
      " |-- device_type: integer (nullable = true)\n",
      " |-- device_conn_type: integer (nullable = true)\n",
      " |-- C14: integer (nullable = true)\n",
      " |-- C15: integer (nullable = true)\n",
      " |-- C16: integer (nullable = true)\n",
      " |-- C17: integer (nullable = true)\n",
      " |-- C18: integer (nullable = true)\n",
      " |-- C19: integer (nullable = true)\n",
      " |-- C20: integer (nullable = true)\n",
      " |-- C21: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# raw_training_data = raw_training_data.withColumnRenamed(\"click\", \"label\")\n",
    "\n",
    "raw_training_data.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a46e22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7405bb53",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c585721c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e472df90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc5a0c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a420b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb578139",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6115191c-d5cb-4279-b6f5-400f1b36337b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-02T17:07:36.192715Z",
     "iopub.status.busy": "2022-06-02T17:07:36.192374Z",
     "iopub.status.idle": "2022-06-02T17:07:36.237606Z",
     "shell.execute_reply": "2022-06-02T17:07:36.234751Z",
     "shell.execute_reply.started": "2022-06-02T17:07:36.192693Z"
    }
   },
   "outputs": [],
   "source": [
    "raw_training_data.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd62740a-fda6-466b-8f79-c9cb56d1e8fa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-02T17:07:37.222278Z",
     "iopub.status.busy": "2022-06-02T17:07:37.221721Z",
     "iopub.status.idle": "2022-06-02T17:07:37.659019Z",
     "shell.execute_reply": "2022-06-02T17:07:37.658093Z",
     "shell.execute_reply.started": "2022-06-02T17:07:37.222236Z"
    }
   },
   "outputs": [],
   "source": [
    "# training = training.select([col for col in training.columns if col != \"id\"])\n",
    "raw_training_data.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a509ee24",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_training_data.describe().toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "950a50cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get string columns\n",
    "for name, type in raw_training_data.dtypes:\n",
    "  print(name, type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c756d2ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.ml.feature import Imputer\n",
    "\n",
    "# imputer = Imputer(\n",
    "#   inputCols=[\"Glucose\",\"BloodPressure\",\"SkinThickness\",\"BMI\",\"Insulin\"], \n",
    "#   outputCols=[\"Glucose\",\"BloodPressure\",\"SkinThickness\",\"BMI\",\"Insulin\"]\n",
    "# )\n",
    "\n",
    "# model = imputer.fit(raw_training_data)\n",
    "# raw_training_data = model.transform(raw_training_data)\n",
    "\n",
    "# raw_training_data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3c8000",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "cols = []\n",
    "pipeline_stages = []\n",
    "feature_columns = []\n",
    "\n",
    "for name, type in raw_training_data.dtypes:\n",
    "    if type == \"string\":\n",
    "        feature_columns.append(f\"{name}Index\")\n",
    "        pipeline_stages.append(StringIndexer(inputCol=name, outputCol=f\"{name}Index\"))\n",
    "    \n",
    "        cols.append(f\"{name}Index\")\n",
    "    \n",
    "    else:\n",
    "        cols.append(name)\n",
    "\n",
    "print(feature_columns)\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "raw_training_data = Pipeline(stages=pipeline_stages).fit(raw_training_data).transform(raw_training_data)\n",
    "\n",
    "raw_training_data = raw_training_data.select(cols)\n",
    "\n",
    "raw_training_data.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f19ff92",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b503ef05",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder\n",
    "\n",
    "encoder = OneHotEncoder(inputCols=feature_columns,\n",
    "                        outputCols=[col+\"_ohe\" for col in feature_columns])\n",
    "\n",
    "model = encoder.fit(raw_training_data)\n",
    "encoded = model.transform(raw_training_data)\n",
    "\n",
    "encoded.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f44d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = encoded.columns\n",
    "\n",
    "for col_to_remove in feature_columns:\n",
    "  cols.remove(col_to_remove)\n",
    "\n",
    "cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2853be9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cols=raw_training_data.columns\n",
    "cols.remove(\"id\")\n",
    "cols.remove(\"label\")\n",
    "\n",
    "# Let us import the vector assembler\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "assembler = VectorAssembler(inputCols=cols, outputCol=\"features\")\n",
    "\n",
    "# Now let us use the transform method to transform our dataset\n",
    "encoded = assembler.transform(encoded)\n",
    "encoded.select(\"features\").toPandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe27cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StandardScaler\n",
    "\n",
    "standardscaler = StandardScaler().setInputCol(\"features\").setOutputCol(\"Scaled_features\")\n",
    "encoded = standardscaler.fit(encoded).transform(encoded)\n",
    "\n",
    "encoded.select(\"features\",\"Scaled_features\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96cf42a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression(labelCol=\"label\", maxIter=10, regParam= 0.01)\n",
    "\n",
    "model=lr.fit(encoded)\n",
    "\n",
    "predict_train=model.transform(encoded)\n",
    "# predict_test=model.transform(test)\n",
    "predict_train.select(\"label\", \"prediction\", \"probability\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb02c31a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.transform(encoded).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed0d70c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ad3110",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "237d8df9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1c8d15",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2873c16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c3b1ff7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # raw_training_data = raw_training_data.limit(1000)\n",
    "\n",
    "# raw_training_data = raw_training_data.withColumnRenamed(\"click\", \"label\")\n",
    "\n",
    "# raw_training_data = raw_training_data.na.drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "19657367",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.feature import OneHotEncoder\n",
    "from pyspark.ml.feature import OneHotEncoder\n",
    "from pyspark.ml.feature import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "688e00a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the logistic regression model\n",
    "lr = LogisticRegression(maxIter=10, regParam= 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "679a90f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a StringIndexer\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "cols = []\n",
    "pipeline_stages = []\n",
    "feature_columns = []\n",
    "\n",
    "for name, type in raw_training_data.dtypes:\n",
    "    if type == \"string\":\n",
    "        feature_columns.append(f\"{name}Index\")\n",
    "        pipeline_stages.append(StringIndexer(inputCol=name, outputCol=f\"{name}Index\", handleInvalid=\"skip\"))\n",
    "    \n",
    "        cols.append(f\"{name}Index\")\n",
    "    \n",
    "    else:\n",
    "        cols.append(name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "680047ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a one hot encoder\n",
    "feature_columns = ['site_idIndex', 'site_domainIndex', 'site_categoryIndex', 'app_idIndex', 'app_domainIndex', 'app_categoryIndex', 'device_idIndex', 'device_ipIndex', 'device_modelIndex']\n",
    "output_ohe_columns = ['site_id_ohe', 'site_domain_ohe', 'site_category_ohe', 'app_id_ohe', 'app_domain_ohe', 'app_category_ohe', 'device_id_ohe', 'device_ip_ohe', 'device_model_ohe']\n",
    "\n",
    "ohe = OneHotEncoder(inputCols = feature_columns, outputCols = output_ohe_columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1bcaedc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import MinMaxScaler\n",
    "\n",
    "# Input list for scaling\n",
    "inputs = [\"hour\", \"C1\", \"banner_pos\", \"device_type\", \"device_conn_type\", \"C14\", \"C15\", \"C16\", \"C17\", \"C18\", \"C19\", \"C20\", \"C21\"]\n",
    "\n",
    "# We scale our inputs\n",
    "assembler1 = VectorAssembler(inputCols=inputs, outputCol=\"features_scaled1\")\n",
    "scaler = MinMaxScaler(inputCol=\"features_scaled1\", outputCol=\"features_scaled\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6ab4af82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create a second assembler for the encoded columns.\n",
    "assembler2 = VectorAssembler(\n",
    "  inputCols=['features_scaled'] + output_ohe_columns, outputCol=\"features\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fe0fe8e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/06/03 21:18:59 ERROR TaskSchedulerImpl: Lost executor 0 on 10.32.45.215: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\n",
      "22/06/03 21:18:59 WARN TaskSetManager: Lost task 0.0 in stage 0.0 (TID 0) (10.32.45.215 executor 0): ExecutorLostFailure (executor 0 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\n",
      "22/06/03 21:19:02 ERROR TaskSchedulerImpl: Lost executor 1 on 10.32.45.215: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\n",
      "22/06/03 21:19:02 WARN TaskSetManager: Lost task 0.1 in stage 0.0 (TID 1) (10.32.45.215 executor 1): ExecutorLostFailure (executor 1 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\n",
      "22/06/03 21:19:05 ERROR TaskSchedulerImpl: Lost executor 2 on 10.32.45.215: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\n",
      "22/06/03 21:19:05 WARN TaskSetManager: Lost task 0.2 in stage 0.0 (TID 2) (10.32.45.215 executor 2): ExecutorLostFailure (executor 2 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\n",
      "22/06/03 21:19:07 ERROR TaskSchedulerImpl: Lost executor 3 on 10.32.45.215: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\n",
      "22/06/03 21:19:07 WARN TaskSetManager: Lost task 0.3 in stage 0.0 (TID 3) (10.32.45.215 executor 3): ExecutorLostFailure (executor 3 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\n",
      "22/06/03 21:19:07 ERROR TaskSetManager: Task 0 in stage 0.0 failed 4 times; aborting job\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o56.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 4 times, most recent failure: Lost task 0.3 in stage 0.0 (TID 3) (10.32.45.215 executor 3): ExecutorLostFailure (executor 3 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2403)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2352)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2351)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2351)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1109)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1109)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1109)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2591)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2533)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2522)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m/home/kayser/git/data-pipelines-with-apache-kafka/spark-ml-training/Spark ML Training.ipynb Cell 40'\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B10.32.45.215/home/kayser/git/data-pipelines-with-apache-kafka/spark-ml-training/Spark%20ML%20Training.ipynb#ch0000031vscode-remote?line=6'>7</a>\u001b[0m pipeline \u001b[39m=\u001b[39m Pipeline(stages\u001b[39m=\u001b[39m myStages)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B10.32.45.215/home/kayser/git/data-pipelines-with-apache-kafka/spark-ml-training/Spark%20ML%20Training.ipynb#ch0000031vscode-remote?line=8'>9</a>\u001b[0m \u001b[39m# We fit the model using the training data.\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B10.32.45.215/home/kayser/git/data-pipelines-with-apache-kafka/spark-ml-training/Spark%20ML%20Training.ipynb#ch0000031vscode-remote?line=9'>10</a>\u001b[0m pModel \u001b[39m=\u001b[39m pipeline\u001b[39m.\u001b[39;49mfit(raw_training_data)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.32.45.215/home/kayser/git/data-pipelines-with-apache-kafka/spark-ml-training/Spark%20ML%20Training.ipynb#ch0000031vscode-remote?line=11'>12</a>\u001b[0m \u001b[39m# We transform the data.\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.32.45.215/home/kayser/git/data-pipelines-with-apache-kafka/spark-ml-training/Spark%20ML%20Training.ipynb#ch0000031vscode-remote?line=12'>13</a>\u001b[0m trainingPred \u001b[39m=\u001b[39m pModel\u001b[39m.\u001b[39mtransform(raw_training_data)\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/spark-ml-training-Z1-rjbZ7-py3.9/lib/python3.9/site-packages/pyspark/ml/base.py:161\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    <a href='file:///home/kayser/.cache/pypoetry/virtualenvs/spark-ml-training-Z1-rjbZ7-py3.9/lib/python3.9/site-packages/pyspark/ml/base.py?line=158'>159</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcopy(params)\u001b[39m.\u001b[39m_fit(dataset)\n\u001b[1;32m    <a href='file:///home/kayser/.cache/pypoetry/virtualenvs/spark-ml-training-Z1-rjbZ7-py3.9/lib/python3.9/site-packages/pyspark/ml/base.py?line=159'>160</a>\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///home/kayser/.cache/pypoetry/virtualenvs/spark-ml-training-Z1-rjbZ7-py3.9/lib/python3.9/site-packages/pyspark/ml/base.py?line=160'>161</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit(dataset)\n\u001b[1;32m    <a href='file:///home/kayser/.cache/pypoetry/virtualenvs/spark-ml-training-Z1-rjbZ7-py3.9/lib/python3.9/site-packages/pyspark/ml/base.py?line=161'>162</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    <a href='file:///home/kayser/.cache/pypoetry/virtualenvs/spark-ml-training-Z1-rjbZ7-py3.9/lib/python3.9/site-packages/pyspark/ml/base.py?line=162'>163</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mParams must be either a param map or a list/tuple of param maps, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///home/kayser/.cache/pypoetry/virtualenvs/spark-ml-training-Z1-rjbZ7-py3.9/lib/python3.9/site-packages/pyspark/ml/base.py?line=163'>164</a>\u001b[0m                     \u001b[39m\"\u001b[39m\u001b[39mbut got \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m \u001b[39mtype\u001b[39m(params))\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/spark-ml-training-Z1-rjbZ7-py3.9/lib/python3.9/site-packages/pyspark/ml/pipeline.py:114\u001b[0m, in \u001b[0;36mPipeline._fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    <a href='file:///home/kayser/.cache/pypoetry/virtualenvs/spark-ml-training-Z1-rjbZ7-py3.9/lib/python3.9/site-packages/pyspark/ml/pipeline.py?line=111'>112</a>\u001b[0m     dataset \u001b[39m=\u001b[39m stage\u001b[39m.\u001b[39mtransform(dataset)\n\u001b[1;32m    <a href='file:///home/kayser/.cache/pypoetry/virtualenvs/spark-ml-training-Z1-rjbZ7-py3.9/lib/python3.9/site-packages/pyspark/ml/pipeline.py?line=112'>113</a>\u001b[0m \u001b[39melse\u001b[39;00m:  \u001b[39m# must be an Estimator\u001b[39;00m\n\u001b[0;32m--> <a href='file:///home/kayser/.cache/pypoetry/virtualenvs/spark-ml-training-Z1-rjbZ7-py3.9/lib/python3.9/site-packages/pyspark/ml/pipeline.py?line=113'>114</a>\u001b[0m     model \u001b[39m=\u001b[39m stage\u001b[39m.\u001b[39;49mfit(dataset)\n\u001b[1;32m    <a href='file:///home/kayser/.cache/pypoetry/virtualenvs/spark-ml-training-Z1-rjbZ7-py3.9/lib/python3.9/site-packages/pyspark/ml/pipeline.py?line=114'>115</a>\u001b[0m     transformers\u001b[39m.\u001b[39mappend(model)\n\u001b[1;32m    <a href='file:///home/kayser/.cache/pypoetry/virtualenvs/spark-ml-training-Z1-rjbZ7-py3.9/lib/python3.9/site-packages/pyspark/ml/pipeline.py?line=115'>116</a>\u001b[0m     \u001b[39mif\u001b[39;00m i \u001b[39m<\u001b[39m indexOfLastEstimator:\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/spark-ml-training-Z1-rjbZ7-py3.9/lib/python3.9/site-packages/pyspark/ml/base.py:161\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    <a href='file:///home/kayser/.cache/pypoetry/virtualenvs/spark-ml-training-Z1-rjbZ7-py3.9/lib/python3.9/site-packages/pyspark/ml/base.py?line=158'>159</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcopy(params)\u001b[39m.\u001b[39m_fit(dataset)\n\u001b[1;32m    <a href='file:///home/kayser/.cache/pypoetry/virtualenvs/spark-ml-training-Z1-rjbZ7-py3.9/lib/python3.9/site-packages/pyspark/ml/base.py?line=159'>160</a>\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///home/kayser/.cache/pypoetry/virtualenvs/spark-ml-training-Z1-rjbZ7-py3.9/lib/python3.9/site-packages/pyspark/ml/base.py?line=160'>161</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit(dataset)\n\u001b[1;32m    <a href='file:///home/kayser/.cache/pypoetry/virtualenvs/spark-ml-training-Z1-rjbZ7-py3.9/lib/python3.9/site-packages/pyspark/ml/base.py?line=161'>162</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    <a href='file:///home/kayser/.cache/pypoetry/virtualenvs/spark-ml-training-Z1-rjbZ7-py3.9/lib/python3.9/site-packages/pyspark/ml/base.py?line=162'>163</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mParams must be either a param map or a list/tuple of param maps, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///home/kayser/.cache/pypoetry/virtualenvs/spark-ml-training-Z1-rjbZ7-py3.9/lib/python3.9/site-packages/pyspark/ml/base.py?line=163'>164</a>\u001b[0m                     \u001b[39m\"\u001b[39m\u001b[39mbut got \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m \u001b[39mtype\u001b[39m(params))\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/spark-ml-training-Z1-rjbZ7-py3.9/lib/python3.9/site-packages/pyspark/ml/wrapper.py:335\u001b[0m, in \u001b[0;36mJavaEstimator._fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    <a href='file:///home/kayser/.cache/pypoetry/virtualenvs/spark-ml-training-Z1-rjbZ7-py3.9/lib/python3.9/site-packages/pyspark/ml/wrapper.py?line=333'>334</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_fit\u001b[39m(\u001b[39mself\u001b[39m, dataset):\n\u001b[0;32m--> <a href='file:///home/kayser/.cache/pypoetry/virtualenvs/spark-ml-training-Z1-rjbZ7-py3.9/lib/python3.9/site-packages/pyspark/ml/wrapper.py?line=334'>335</a>\u001b[0m     java_model \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit_java(dataset)\n\u001b[1;32m    <a href='file:///home/kayser/.cache/pypoetry/virtualenvs/spark-ml-training-Z1-rjbZ7-py3.9/lib/python3.9/site-packages/pyspark/ml/wrapper.py?line=335'>336</a>\u001b[0m     model \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_create_model(java_model)\n\u001b[1;32m    <a href='file:///home/kayser/.cache/pypoetry/virtualenvs/spark-ml-training-Z1-rjbZ7-py3.9/lib/python3.9/site-packages/pyspark/ml/wrapper.py?line=336'>337</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_copyValues(model)\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/spark-ml-training-Z1-rjbZ7-py3.9/lib/python3.9/site-packages/pyspark/ml/wrapper.py:332\u001b[0m, in \u001b[0;36mJavaEstimator._fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    <a href='file:///home/kayser/.cache/pypoetry/virtualenvs/spark-ml-training-Z1-rjbZ7-py3.9/lib/python3.9/site-packages/pyspark/ml/wrapper.py?line=317'>318</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/kayser/.cache/pypoetry/virtualenvs/spark-ml-training-Z1-rjbZ7-py3.9/lib/python3.9/site-packages/pyspark/ml/wrapper.py?line=318'>319</a>\u001b[0m \u001b[39mFits a Java model to the input dataset.\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/kayser/.cache/pypoetry/virtualenvs/spark-ml-training-Z1-rjbZ7-py3.9/lib/python3.9/site-packages/pyspark/ml/wrapper.py?line=319'>320</a>\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///home/kayser/.cache/pypoetry/virtualenvs/spark-ml-training-Z1-rjbZ7-py3.9/lib/python3.9/site-packages/pyspark/ml/wrapper.py?line=328'>329</a>\u001b[0m \u001b[39m    fitted Java model\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/kayser/.cache/pypoetry/virtualenvs/spark-ml-training-Z1-rjbZ7-py3.9/lib/python3.9/site-packages/pyspark/ml/wrapper.py?line=329'>330</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/kayser/.cache/pypoetry/virtualenvs/spark-ml-training-Z1-rjbZ7-py3.9/lib/python3.9/site-packages/pyspark/ml/wrapper.py?line=330'>331</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_transfer_params_to_java()\n\u001b[0;32m--> <a href='file:///home/kayser/.cache/pypoetry/virtualenvs/spark-ml-training-Z1-rjbZ7-py3.9/lib/python3.9/site-packages/pyspark/ml/wrapper.py?line=331'>332</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_java_obj\u001b[39m.\u001b[39;49mfit(dataset\u001b[39m.\u001b[39;49m_jdf)\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/spark-ml-training-Z1-rjbZ7-py3.9/lib/python3.9/site-packages/py4j/java_gateway.py:1309\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   <a href='file:///home/kayser/.cache/pypoetry/virtualenvs/spark-ml-training-Z1-rjbZ7-py3.9/lib/python3.9/site-packages/py4j/java_gateway.py?line=1302'>1303</a>\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[1;32m   <a href='file:///home/kayser/.cache/pypoetry/virtualenvs/spark-ml-training-Z1-rjbZ7-py3.9/lib/python3.9/site-packages/py4j/java_gateway.py?line=1303'>1304</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[1;32m   <a href='file:///home/kayser/.cache/pypoetry/virtualenvs/spark-ml-training-Z1-rjbZ7-py3.9/lib/python3.9/site-packages/py4j/java_gateway.py?line=1304'>1305</a>\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[1;32m   <a href='file:///home/kayser/.cache/pypoetry/virtualenvs/spark-ml-training-Z1-rjbZ7-py3.9/lib/python3.9/site-packages/py4j/java_gateway.py?line=1305'>1306</a>\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   <a href='file:///home/kayser/.cache/pypoetry/virtualenvs/spark-ml-training-Z1-rjbZ7-py3.9/lib/python3.9/site-packages/py4j/java_gateway.py?line=1307'>1308</a>\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[0;32m-> <a href='file:///home/kayser/.cache/pypoetry/virtualenvs/spark-ml-training-Z1-rjbZ7-py3.9/lib/python3.9/site-packages/py4j/java_gateway.py?line=1308'>1309</a>\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[1;32m   <a href='file:///home/kayser/.cache/pypoetry/virtualenvs/spark-ml-training-Z1-rjbZ7-py3.9/lib/python3.9/site-packages/py4j/java_gateway.py?line=1309'>1310</a>\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[1;32m   <a href='file:///home/kayser/.cache/pypoetry/virtualenvs/spark-ml-training-Z1-rjbZ7-py3.9/lib/python3.9/site-packages/py4j/java_gateway.py?line=1311'>1312</a>\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[1;32m   <a href='file:///home/kayser/.cache/pypoetry/virtualenvs/spark-ml-training-Z1-rjbZ7-py3.9/lib/python3.9/site-packages/py4j/java_gateway.py?line=1312'>1313</a>\u001b[0m     temp_arg\u001b[39m.\u001b[39m_detach()\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/spark-ml-training-Z1-rjbZ7-py3.9/lib/python3.9/site-packages/pyspark/sql/utils.py:111\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    <a href='file:///home/kayser/.cache/pypoetry/virtualenvs/spark-ml-training-Z1-rjbZ7-py3.9/lib/python3.9/site-packages/pyspark/sql/utils.py?line=108'>109</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdeco\u001b[39m(\u001b[39m*\u001b[39ma, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw):\n\u001b[1;32m    <a href='file:///home/kayser/.cache/pypoetry/virtualenvs/spark-ml-training-Z1-rjbZ7-py3.9/lib/python3.9/site-packages/pyspark/sql/utils.py?line=109'>110</a>\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///home/kayser/.cache/pypoetry/virtualenvs/spark-ml-training-Z1-rjbZ7-py3.9/lib/python3.9/site-packages/pyspark/sql/utils.py?line=110'>111</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39;49ma, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkw)\n\u001b[1;32m    <a href='file:///home/kayser/.cache/pypoetry/virtualenvs/spark-ml-training-Z1-rjbZ7-py3.9/lib/python3.9/site-packages/pyspark/sql/utils.py?line=111'>112</a>\u001b[0m     \u001b[39mexcept\u001b[39;00m py4j\u001b[39m.\u001b[39mprotocol\u001b[39m.\u001b[39mPy4JJavaError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    <a href='file:///home/kayser/.cache/pypoetry/virtualenvs/spark-ml-training-Z1-rjbZ7-py3.9/lib/python3.9/site-packages/pyspark/sql/utils.py?line=112'>113</a>\u001b[0m         converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/spark-ml-training-Z1-rjbZ7-py3.9/lib/python3.9/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    <a href='file:///home/kayser/.cache/pypoetry/virtualenvs/spark-ml-training-Z1-rjbZ7-py3.9/lib/python3.9/site-packages/py4j/protocol.py?line=323'>324</a>\u001b[0m value \u001b[39m=\u001b[39m OUTPUT_CONVERTER[\u001b[39mtype\u001b[39m](answer[\u001b[39m2\u001b[39m:], gateway_client)\n\u001b[1;32m    <a href='file:///home/kayser/.cache/pypoetry/virtualenvs/spark-ml-training-Z1-rjbZ7-py3.9/lib/python3.9/site-packages/py4j/protocol.py?line=324'>325</a>\u001b[0m \u001b[39mif\u001b[39;00m answer[\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> <a href='file:///home/kayser/.cache/pypoetry/virtualenvs/spark-ml-training-Z1-rjbZ7-py3.9/lib/python3.9/site-packages/py4j/protocol.py?line=325'>326</a>\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    <a href='file:///home/kayser/.cache/pypoetry/virtualenvs/spark-ml-training-Z1-rjbZ7-py3.9/lib/python3.9/site-packages/py4j/protocol.py?line=326'>327</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[1;32m    <a href='file:///home/kayser/.cache/pypoetry/virtualenvs/spark-ml-training-Z1-rjbZ7-py3.9/lib/python3.9/site-packages/py4j/protocol.py?line=327'>328</a>\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name), value)\n\u001b[1;32m    <a href='file:///home/kayser/.cache/pypoetry/virtualenvs/spark-ml-training-Z1-rjbZ7-py3.9/lib/python3.9/site-packages/py4j/protocol.py?line=328'>329</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    <a href='file:///home/kayser/.cache/pypoetry/virtualenvs/spark-ml-training-Z1-rjbZ7-py3.9/lib/python3.9/site-packages/py4j/protocol.py?line=329'>330</a>\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    <a href='file:///home/kayser/.cache/pypoetry/virtualenvs/spark-ml-training-Z1-rjbZ7-py3.9/lib/python3.9/site-packages/py4j/protocol.py?line=330'>331</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m. Trace:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{3}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[1;32m    <a href='file:///home/kayser/.cache/pypoetry/virtualenvs/spark-ml-training-Z1-rjbZ7-py3.9/lib/python3.9/site-packages/py4j/protocol.py?line=331'>332</a>\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o56.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 4 times, most recent failure: Lost task 0.3 in stage 0.0 (TID 3) (10.32.45.215 executor 3): ExecutorLostFailure (executor 3 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2403)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2352)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2351)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2351)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1109)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1109)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1109)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2591)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2533)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2522)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# Create stages list\n",
    "myStages = pipeline_stages + [assembler1, scaler, ohe, assembler2, lr]\n",
    "\n",
    "# Set up the pipeline\n",
    "pipeline = Pipeline(stages= myStages)\n",
    "\n",
    "# We fit the model using the training data.\n",
    "pModel = pipeline.fit(raw_training_data)\n",
    "\n",
    "# We transform the data.\n",
    "trainingPred = pModel.transform(raw_training_data)\n",
    "\n",
    "# # We select the actual label, probability and predictions\n",
    "trainingPred.select('label', 'probability', 'prediction').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "635bc295",
   "metadata": {},
   "outputs": [],
   "source": [
    "pModel.save(\"model/spark-logistic-regression-model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "461169c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = pModel.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3da2151",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "\n",
    "accuracy = evaluator.evaluate(pred)\n",
    "\n",
    "print(\"Train Accuracy = %g \" % (accuracy))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "562fa316",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd92cdc6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6607b8df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3107fb18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.feature import OneHotEncoder\n",
    "from pyspark.ml.feature import OneHotEncoder\n",
    "from pyspark.ml.feature import MinMaxScaler\n",
    "\n",
    "# Create the logistic regression model\n",
    "lr = LogisticRegression(maxIter=10, regParam= 0.01)\n",
    "\n",
    "# We create a one hot encoder.\n",
    "# ohe = OneHotEncoder(\n",
    "#   # inputCols = [\"site_id\", \"site_domain\", \"site_category\", \"app_id\", \"app_domain\", \"app_category\", \"device_id\", \"device_ip\", \"device_model\"], \n",
    "#   inputCols = [\"site_domain\", \"site_category\", \"app_id\", \"app_domain\", \"app_category\", \"device_id\", \"device_ip\", \"device_model\"], \n",
    "#   # outputCols=[\"site_id_ohe\", \"site_domain_ohe\", \"site_category_ohe\", \"app_id_ohe\", \"app_domain_ohe\", \"app_category_ohe\", \"device_id_ohe\", \"device_ip_ohe\", \"device_model_ohe\"]\n",
    "#   outputCols=[\"site_domain_ohe\", \"site_category_ohe\", \"app_id_ohe\", \"app_domain_ohe\", \"app_category_ohe\", \"device_id_ohe\", \"device_ip_ohe\", \"device_model_ohe\"]\n",
    "# )\n",
    "\n",
    "# Input list for scaling\n",
    "inputs = [\"hour\", \"C1\", \"banner_pos\", \"device_type\", \"device_conn_type\", \"C14\", \"C15\", \"C16\", \"C17\", \"C18\", \"C19\", \"C20\", \"C21\"]\n",
    "\n",
    "# We scale our inputs\n",
    "assembler1 = VectorAssembler(inputCols=inputs, outputCol=\"features_scaled1\")\n",
    "scaler = MinMaxScaler(inputCol=\"features_scaled1\", outputCol=\"features_scaled\")\n",
    "\n",
    "# We create a second assembler for the encoded columns.\n",
    "assembler2 = VectorAssembler(\n",
    "  # inputCols=[\"site_id_ohe\", \"site_domain_ohe\", \"site_category_ohe\", \"app_id_ohe\", \"app_domain_ohe\", \"app_category_ohe\", \"device_id_ohe\", \"device_ip_ohe\", \"device_model_ohe\", 'features_scaled'], outputCol=\"features\"\n",
    "  inputCols=[\"site_domain\", \"site_category\", \"app_id\", \"app_domain\", \"app_category\", \"device_id\", \"device_ip\", \"device_model\", 'features_scaled'], outputCol=\"features\"\n",
    ")\n",
    "\n",
    "# Create stages list\n",
    "# myStages = [assembler1, scaler, ohe, assembler2,lr]\n",
    "myStages = [assembler1, scaler, assembler2, lr]\n",
    "\n",
    "# Set up the pipeline\n",
    "pipeline = Pipeline(stages= myStages)\n",
    "\n",
    "# We fit the model using the training data.\n",
    "pModel = pipeline.fit(raw_training_data)\n",
    "\n",
    "# We transform the data.\n",
    "trainingPred = pModel.transform(raw_training_data)\n",
    "\n",
    "# # We select the actual label, probability and predictions\n",
    "trainingPred.select('label', 'probability', 'prediction').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc58b80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3168238c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c995bc84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aecd2576",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "996e3382",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a62b91b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fbbe97c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f58f384b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2445a7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b55164",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-02T17:07:37.660549Z",
     "iopub.status.busy": "2022-06-02T17:07:37.660168Z",
     "iopub.status.idle": "2022-06-02T17:07:37.888200Z",
     "shell.execute_reply": "2022-06-02T17:07:37.887069Z",
     "shell.execute_reply.started": "2022-06-02T17:07:37.660516Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807700b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d58bf8e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = raw_training_data.columns\n",
    "cols.remove(\"label\")\n",
    "\n",
    "# Let us import the vector assembler\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "assembler = VectorAssembler(inputCols=cols, outputCol=\"features\")\n",
    "\n",
    "# Now let us use the transform method to transform our dataset\n",
    "raw_training_data = assembler.transform(raw_training_data)\n",
    "raw_training_data.select(\"features\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb1505ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = raw_training_data.select(\"features\", \"label\") #.withColumnRenamed(\"click\", \"label\")\n",
    "training_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf6b58f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression(maxIter=10, regParam=0.3, elasticNetParam=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b79ef0a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = lr.fit(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c29fab7c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f344cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = spark.read.format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .load('../dataset/click-through-rate-prediction/test.gz')\n",
    "\n",
    "test.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa585a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ba9425",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, FloatType, StringType, LongType, IntegerType\n",
    "\n",
    "schema = StructType([\n",
    "    StructField('id', LongType(), True),\n",
    "    StructField(\"hour\", IntegerType(), True),\n",
    "    StructField(\"C1\", IntegerType(), True),\n",
    "    StructField(\"banner_pos\", IntegerType(), True),\n",
    "    StructField(\"site_id\", StringType(), True),\n",
    "    StructField(\"site_domain\", StringType(), True),\n",
    "    StructField(\"site_category\", StringType(), True),\n",
    "    StructField(\"app_id\", StringType(), True),\n",
    "    StructField(\"app_domain\", StringType(), True),\n",
    "    StructField(\"app_category\", StringType(), True),\n",
    "    StructField(\"device_id\", StringType(), True),\n",
    "    StructField(\"device_ip\", StringType(), True),\n",
    "    StructField(\"device_model\", StringType(), True),\n",
    "    StructField(\"device_type\", IntegerType(), True),\n",
    "    StructField(\"device_conn_type\", IntegerType(), True),\n",
    "    StructField(\"C14\", IntegerType(), True),\n",
    "    StructField(\"C15\", IntegerType(), True),\n",
    "    StructField(\"C16\", IntegerType(), True),\n",
    "    StructField(\"C17\", IntegerType(), True),\n",
    "    StructField(\"C18\", IntegerType(), True),\n",
    "    StructField(\"C19\", IntegerType(), True),\n",
    "    StructField(\"C20\", IntegerType(), True),\n",
    "    StructField(\"C21\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "# Load test data\n",
    "test = spark.read.format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .schema(schema) \\\n",
    "    .load('../dataset/click-through-rate-prediction/test.gz')\n",
    "\n",
    "test = test.withColumnRenamed(\"click\", \"label\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be9f85b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = pModel.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a906ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bbb2cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "\n",
    "accuracy = evaluator.evaluate(pred)\n",
    "\n",
    "print(\"Train Accuracy = %g \" % (accuracy))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3aa3ae8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b72e8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fe3cb5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7772c1da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "cols = []\n",
    "pipeline_stages = []\n",
    "feature_columns = []\n",
    "\n",
    "for name, type in test.dtypes:\n",
    "    if type == \"string\":\n",
    "        feature_columns.append(f\"{name}Index\")\n",
    "        pipeline_stages.append(StringIndexer(inputCol=name, outputCol=f\"{name}Index\"))\n",
    "    \n",
    "        cols.append(f\"{name}Index\")\n",
    "    \n",
    "    else:\n",
    "        cols.append(name)\n",
    "\n",
    "cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde70f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae9f670",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "\n",
    "test = Pipeline(stages=pipeline_stages).fit(test).transform(test)\n",
    "\n",
    "test = test.select(cols)\n",
    "\n",
    "test.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d625e5b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = test.columns\n",
    "# cols.remove(\"click\")\n",
    "\n",
    "# Let us import the vector assembler\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "assembler = VectorAssembler(inputCols=cols, outputCol=\"features\")\n",
    "\n",
    "# Now let us use the transform method to transform our dataset\n",
    "test = assembler.transform(test)\n",
    "test = test.select(\"features\")\n",
    "\n",
    "test.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb12eb59",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc8be501",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5137f3a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae2f827",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StandardScaler\n",
    "\n",
    "standardscaler = StandardScaler().setInputCol(\"features\").setOutputCol(\"Scaled_features\")\n",
    "raw_training_data = standardscaler.fit(raw_training_data).transform(raw_training_data)\n",
    "raw_training_data.select(\"features\",\"Scaled_features\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "652a895e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b39e881b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fbd0437",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc282ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d13dfa0-61a3-4d6c-93f0-60d1c35b6704",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-02T17:07:37.891601Z",
     "iopub.status.busy": "2022-06-02T17:07:37.889907Z",
     "iopub.status.idle": "2022-06-02T17:07:37.903945Z",
     "shell.execute_reply": "2022-06-02T17:07:37.899302Z",
     "shell.execute_reply.started": "2022-06-02T17:07:37.891544Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed9417e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-02T17:07:37.906020Z",
     "iopub.status.busy": "2022-06-02T17:07:37.905364Z",
     "iopub.status.idle": "2022-06-02T17:07:37.930800Z",
     "shell.execute_reply": "2022-06-02T17:07:37.930219Z",
     "shell.execute_reply.started": "2022-06-02T17:07:37.905972Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import HashingTF, Tokenizer, VectorAssembler\n",
    "\n",
    "# Creating Vector Assembler\n",
    "vecAssembler = VectorAssembler(inputCols=[col if type != \"string\" else col+\"Index\" for col, type in training.dtypes if col != \"id\"], outputCol=\"features\")\n",
    "pipeline_stages.append(vecAssembler)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0048659c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-02T17:07:37.932265Z",
     "iopub.status.busy": "2022-06-02T17:07:37.932059Z",
     "iopub.status.idle": "2022-06-02T17:07:38.054400Z",
     "shell.execute_reply": "2022-06-02T17:07:38.053236Z",
     "shell.execute_reply.started": "2022-06-02T17:07:37.932249Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# Creating Logistic Regression Model\n",
    "lr = LogisticRegression(maxIter=10, regParam=0.3, elasticNetParam=0.8)\n",
    "pipeline_stages.append(lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f9ef2de",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-02T17:07:38.057369Z",
     "iopub.status.busy": "2022-06-02T17:07:38.055375Z",
     "iopub.status.idle": "2022-06-02T17:07:38.076242Z",
     "shell.execute_reply": "2022-06-02T17:07:38.060556Z",
     "shell.execute_reply.started": "2022-06-02T17:07:38.057334Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# Creating pipeline\n",
    "pipeline = Pipeline(stages=pipeline_stages)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152af103",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-02T17:07:38.078057Z",
     "iopub.status.busy": "2022-06-02T17:07:38.077615Z",
     "iopub.status.idle": "2022-06-02T17:22:17.399769Z",
     "shell.execute_reply": "2022-06-02T17:22:17.398751Z",
     "shell.execute_reply.started": "2022-06-02T17:07:38.078022Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Executing pipeline (VectorAssembler + LogisticRegression)\n",
    "lrModel = pipeline.fit(training)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed50f865",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "646a395b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa23e60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e65936",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e19dcc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00134877",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6908e48f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e319d2a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b39ba777",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Configure an ML pipeline, which consists of three stages: tokenizer, hashingTF, and lr.\n",
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "hashingTF = HashingTF(inputCol=tokenizer.getOutputCol(), outputCol=\"features\")\n",
    "lr = LogisticRegression(maxIter=10, regParam=0.001)\n",
    "\n",
    "pipeline = Pipeline(stages=[tokenizer, hashingTF, lr])\n",
    "\n",
    "# Fit the pipeline to training documents.\n",
    "model = pipeline.fit(training)\n",
    "\n",
    "# Prepare test documents, which are unlabeled (id, text) tuples.\n",
    "test = spark.createDataFrame([\n",
    "    (4, \"spark i j k\"),\n",
    "    (5, \"l m n\"),\n",
    "    (6, \"spark hadoop spark\"),\n",
    "    (7, \"apache hadoop\")\n",
    "], [\"id\", \"text\"])\n",
    "\n",
    "# Make predictions on test documents and print columns of interest.\n",
    "prediction = model.transform(test)\n",
    "selected = prediction.select(\"id\", \"text\", \"probability\", \"prediction\")\n",
    "for row in selected.collect():\n",
    "    rid, text, prob, prediction = row  # type: ignore\n",
    "    print(\n",
    "        \"(%d, %s) --> prob=%s, prediction=%f\" % (\n",
    "            rid, text, str(prob), prediction   # type: ignore\n",
    "        )\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8457bb32",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression(maxIter=10, regParam=0.3, elasticNetParam=0.8)\n",
    "\n",
    "# Fit the model\n",
    "lrModel = lr.fit(training)\n",
    "\n",
    "# Print the coefficients and intercept for logistic regression\n",
    "print(\"Coefficients: \" + str(lrModel.coefficients))\n",
    "print(\"Intercept: \" + str(lrModel.intercept))\n",
    "\n",
    "# We can also use the multinomial family for binary classification\n",
    "mlr = LogisticRegression(maxIter=10, regParam=0.3, elasticNetParam=0.8, family=\"multinomial\")\n",
    "\n",
    "# Fit the model\n",
    "mlrModel = mlr.fit(training)\n",
    "\n",
    "# Print the coefficients and intercepts for logistic regression with multinomial family\n",
    "print(\"Multinomial coefficients: \" + str(mlrModel.coefficientMatrix))\n",
    "print(\"Multinomial intercepts: \" + str(mlrModel.interceptVector))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae51eef9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "23290619d6f4f5ecea54fd3814d63b8a6a2d2c019f5870989b08cbcfb848aa36"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 ('spark-ml-training-Z1-rjbZ7-py3.9')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
