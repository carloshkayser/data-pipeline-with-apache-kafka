{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f794456",
   "metadata": {},
   "source": [
    "# Spark Structured Streaming Application\n",
    "\n",
    "This notebook contains ... TODO\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6188f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install pre-requisites\n",
    "!pip install ipython-sql psycopg2-binary pyspark==3.2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9fa01d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load sql magic function\n",
    "%load_ext sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b478262e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import (\n",
    "    StructType,\n",
    "    StructField,\n",
    "    FloatType,\n",
    "    StringType,\n",
    "    LongType,\n",
    "    IntegerType,\n",
    "    DecimalType,\n",
    ")\n",
    "from pyspark.sql.types import (\n",
    "    StructType,\n",
    "    StructField,\n",
    "    FloatType,\n",
    "    StringType,\n",
    "    LongType,\n",
    "    IntegerType,\n",
    "    DoubleType,\n",
    ")\n",
    "from pyspark.sql.functions import (\n",
    "    split,\n",
    "    regexp_replace,\n",
    "    current_date,\n",
    "    unix_timestamp,\n",
    "    lit,\n",
    "    current_timestamp,\n",
    ")\n",
    "\n",
    "from pyspark.sql.functions import col, from_json, struct, to_json\n",
    "from pyspark.sql.functions import pandas_udf, PandasUDFType\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import __version__\n",
    "\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import json\n",
    "import time\n",
    "import os\n",
    "\n",
    "print(\"PySpark\", __version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c5d70ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get kafka broker list from minikube\n",
    "KAFKA_HOST = os.popen(\"minikube service kafka-cluster-kafka-external-bootstrap --url -n demo\").read()\n",
    "print(\"Apache Kafka broker running on:\", KAFKA_HOST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ba7b97",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-06T17:04:27.014515Z",
     "iopub.status.busy": "2022-06-06T17:04:27.013980Z",
     "iopub.status.idle": "2022-06-06T17:04:33.367796Z",
     "shell.execute_reply": "2022-06-06T17:04:33.367050Z",
     "shell.execute_reply.started": "2022-06-06T17:04:27.014449Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.environ[\n",
    "    \"PYSPARK_SUBMIT_ARGS\"\n",
    "] = \"--packages org.apache.spark:spark-streaming-kafka-0-10_2.12:3.2.0,org.apache.spark:spark-sql-kafka-0-10_2.12:3.2.0,org.postgresql:postgresql:42.1.1 pyspark-shell\"\n",
    "\n",
    "\n",
    "APP_NAME = os.getenv(\"APP_NAME\", \"spark-streaming-app\")\n",
    "# MASTER = os.getenv(\"MASTER\", \"local[*]\")\n",
    "MASTER = \"spark://carloshkayser:7077\"\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"Spark Structured Streaming Application\")\n",
    "    .master(MASTER)\n",
    "    # .config(\"spark.files\", \"/home/kayser/git/data-pipelines-with-apache-kafka/spark-ml-training/model\")\n",
    "    .config(\"spark.archives\", \"/home/kayser/git/data-pipelines-with-apache-kafka/spark-ml-training/model/spark-logistic-regression-model.zip\")\n",
    "    # .config(\"spark.metrics.conf\", \"metrics.properties\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "# .config(\"spark.pyspark.python\", \"/home/linuxbrew/.linuxbrew/opt/python@3.10/bin/python3\")\n",
    "# .config(\"spark.pyspark.driver.python\", \"/home/linuxbrew/.linuxbrew/bin/ipython\")\n",
    "\n",
    "spark.sparkContext.setLogLevel('ERROR')\n",
    "\n",
    "spark.conf.set(\"spark.sql.streaming.metricsEnabled\", \"true\")\n",
    "\n",
    "spark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8fe8a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.sql.streaming import StreamingQueryListener\n",
    "\n",
    "\n",
    "# class MyListener(StreamingQueryListener):\n",
    "#     def onQueryStarted(self, event):\n",
    "#         \"\"\"\n",
    "#         Called when a query is started.\n",
    "\n",
    "#         Parameters\n",
    "#         ----------\n",
    "#         event: :class:`pyspark.sql.streaming.listener.QueryStartedEvent`\n",
    "#             The properties are available as the same as Scala API.\n",
    "\n",
    "#         Notes\n",
    "#         -----\n",
    "#         This is called synchronously with\n",
    "#         meth:`pyspark.sql.streaming.DataStreamWriter.start`,\n",
    "#         that is, ``onQueryStart`` will be called on all listeners before\n",
    "#         ``DataStreamWriter.start()`` returns the corresponding\n",
    "#         :class:`pyspark.sql.streaming.StreamingQuery`.\n",
    "#         Do not block in this method as it will block your query.\n",
    "#         \"\"\"\n",
    "#         print(\"Query started:\" + event.id)\n",
    "\n",
    "#     def onQueryProgress(self, event):\n",
    "#         \"\"\"\n",
    "#         Called when there is some status update (ingestion rate updated, etc.)\n",
    "\n",
    "#         Parameters\n",
    "#         ----------\n",
    "#         event: :class:`pyspark.sql.streaming.listener.QueryProgressEvent`\n",
    "#             The properties are available as the same as Scala API.\n",
    "\n",
    "#         Notes\n",
    "#         -----\n",
    "#         This method is asynchronous. The status in\n",
    "#         :class:`pyspark.sql.streaming.StreamingQuery` will always be\n",
    "#         latest no matter when this method is called. Therefore, the status\n",
    "#         of :class:`pyspark.sql.streaming.StreamingQuery`.\n",
    "#         may be changed before/when you process the event.\n",
    "#         For example, you may find :class:`StreamingQuery`\n",
    "#         is terminated when you are processing `QueryProgressEvent`.\n",
    "#         \"\"\"\n",
    "\n",
    "#         print(\"Query made progress\")\n",
    "      \n",
    "#         # getting current batch information in json format\n",
    "#         jsonData = event.progress.json\n",
    "\n",
    "#         print(jsonData)\n",
    "        \n",
    "#         # # creating dataset based on json for easier parsing and transforming\n",
    "#         # DF = spark.read.json(spark.createDataset(jsonData))\n",
    "        \n",
    "#         # # selecting relevant columns from dataset\n",
    "#         # DF_flat = DF.select(\n",
    "#         #     \"id\",\n",
    "#         #     \"runId\",\n",
    "#         #     \"name\",\n",
    "#         #     \"timestamp\",\n",
    "#         #     \"batchId\",\n",
    "#         #     \"numInputRows\",\n",
    "#         #     \"inputRowsPerSecond\",\n",
    "#         #     \"processedRowsPerSecond\",\n",
    "#         #     \"durationMs.latestOffset\",\n",
    "#         #     \"durationMs.triggerExecution\",\n",
    "#         #     \"sink.description\",\n",
    "#         #     \"sink.numOutputRows\"\n",
    "#         # )\n",
    "        \n",
    "#         # # transform dataset back to json\n",
    "#         # DF_flat_json = DF_flat.toJSON\n",
    "        \n",
    "#         # # get json as string\n",
    "#         # DF_flat_json_string = DF_flat_json.select(\"value\").collect().map(_.getString(0)).mkString(\" \")      \n",
    "        \n",
    "#         # # build and send http post request to power bi push dataset (created upfront)\n",
    "#         # val url = \"PasteEndpointFromPowerBI\"      \n",
    "#         # val client = HttpClientBuilder.create().build()\n",
    "#         # val post:HttpPost = new HttpPost(url)\n",
    "#         # post.addHeader(\"Content-Type\", \"application/json\")\n",
    "#         # val post_body = new StringEntity(\"[\"+DF_flat_json_string+\"]\")\n",
    "#         # post.setEntity(post_body)\n",
    "#         # val response:CloseableHttpResponse = client.execute(post)\n",
    "\n",
    "#     def onQueryTerminated(self, event):\n",
    "#         \"\"\"\n",
    "#         Called when a query is stopped, with or without error.\n",
    "\n",
    "#         Parameters\n",
    "#         ----------\n",
    "#         event: :class:`pyspark.sql.streaming.listener.QueryTerminatedEvent`\n",
    "#             The properties are available as the same as Scala API.\n",
    "#         \"\"\"\n",
    "#         print(\"Query terminated:\" + event.id)\n",
    "\n",
    "\n",
    "# my_listener = MyListener()\n",
    "\n",
    "# spark.streams.addListener(my_listener)\n",
    "# # spark.streams.removeListener(my_listener)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8286dabe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-06T17:04:33.369282Z",
     "iopub.status.busy": "2022-06-06T17:04:33.368885Z",
     "iopub.status.idle": "2022-06-06T17:04:34.892328Z",
     "shell.execute_reply": "2022-06-06T17:04:34.891611Z",
     "shell.execute_reply.started": "2022-06-06T17:04:33.369251Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_raw = (\n",
    "    spark.readStream.format(\"kafka\")\n",
    "    .option(\"kafka.bootstrap.servers\", KAFKA_HOST)\n",
    "    .option(\"subscribe\", \"to_predict\")\n",
    "    .option(\"startingOffsets\", \"latest\")\n",
    "    .load()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6008327",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-06T17:04:34.893745Z",
     "iopub.status.busy": "2022-06-06T17:04:34.893368Z",
     "iopub.status.idle": "2022-06-06T17:04:34.918896Z",
     "shell.execute_reply": "2022-06-06T17:04:34.918246Z",
     "shell.execute_reply.started": "2022-06-06T17:04:34.893719Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_raw.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d2fb02-a207-410b-9575-2f4077bab353",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-06T17:04:34.921377Z",
     "iopub.status.busy": "2022-06-06T17:04:34.920993Z",
     "iopub.status.idle": "2022-06-06T17:04:35.438137Z",
     "shell.execute_reply": "2022-06-06T17:04:35.436614Z",
     "shell.execute_reply.started": "2022-06-06T17:04:34.921352Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# {\n",
    "#   \"id\": 1.0079274744188029e+19,\n",
    "#   \"hour\": 14103100,\n",
    "#   \"C1\": 1005,\n",
    "#   \"banner_pos\": 0,\n",
    "#   \"site_id\": \"85f751fd\",\n",
    "#   \"site_domain\": \"c4e18dd6\",\n",
    "#   \"site_category\": \"50e219e0\",\n",
    "#   \"app_id\": \"febd1138\",\n",
    "#   \"app_domain\": \"82e27996\",\n",
    "#   \"app_category\": \"0f2161f8\",\n",
    "#   \"device_id\": \"a99f214a\",\n",
    "#   \"device_ip\": \"b72692c8\",\n",
    "#   \"device_model\": \"99e427c9\",\n",
    "#   \"device_type\": 1,\n",
    "#   \"device_conn_type\": 0,\n",
    "#   \"C14\": 21611,\n",
    "#   \"C15\": 320,\n",
    "#   \"C16\": 50,\n",
    "#   \"C17\": 2480,\n",
    "#   \"C18\": 3,\n",
    "#   \"C19\": 299,\n",
    "#   \"C20\": 100111,\n",
    "#   \"C21\": 61\n",
    "# }\n",
    "\n",
    "schema = StructType(\n",
    "    [\n",
    "        StructField(\"id\", DecimalType(38, 0), True),\n",
    "        StructField(\"hour\", IntegerType(), True),\n",
    "        StructField(\"C1\", IntegerType(), True),\n",
    "        StructField(\"banner_pos\", IntegerType(), True),\n",
    "        StructField(\"site_id\", StringType(), True),\n",
    "        StructField(\"site_domain\", StringType(), True),\n",
    "        StructField(\"site_category\", StringType(), True),\n",
    "        StructField(\"app_id\", StringType(), True),\n",
    "        StructField(\"app_domain\", StringType(), True),\n",
    "        StructField(\"app_category\", StringType(), True),\n",
    "        StructField(\"device_id\", StringType(), True),\n",
    "        StructField(\"device_ip\", StringType(), True),\n",
    "        StructField(\"device_model\", StringType(), True),\n",
    "        StructField(\"device_type\", IntegerType(), True),\n",
    "        StructField(\"device_conn_type\", IntegerType(), True),\n",
    "        StructField(\"C14\", IntegerType(), True),\n",
    "        StructField(\"C15\", IntegerType(), True),\n",
    "        StructField(\"C16\", IntegerType(), True),\n",
    "        StructField(\"C17\", IntegerType(), True),\n",
    "        StructField(\"C18\", IntegerType(), True),\n",
    "        StructField(\"C19\", IntegerType(), True),\n",
    "        StructField(\"C20\", IntegerType(), True),\n",
    "        StructField(\"C21\", IntegerType(), True),\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b073d71f-22d7-4634-ac56-dd82968f3b3a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-06T17:04:35.440829Z",
     "iopub.status.busy": "2022-06-06T17:04:35.440219Z",
     "iopub.status.idle": "2022-06-06T17:04:35.831027Z",
     "shell.execute_reply": "2022-06-06T17:04:35.830348Z",
     "shell.execute_reply.started": "2022-06-06T17:04:35.440795Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = (\n",
    "    df_raw.selectExpr(\"CAST(value AS STRING)\")\n",
    "    .select(from_json(\"value\", schema).alias(\"data\"))\n",
    "    .select(\"data.*\")\n",
    ")\n",
    "\n",
    "df.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "536fafac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkFiles\n",
    "\n",
    "pipeline_model_path = SparkFiles.get(\"spark-logistic-regression-model.zip\")\n",
    "pipeline_model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60393740-2f87-4fef-b611-40add528fa27",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-06T17:04:35.832359Z",
     "iopub.status.busy": "2022-06-06T17:04:35.832053Z",
     "iopub.status.idle": "2022-06-06T17:04:43.307993Z",
     "shell.execute_reply": "2022-06-06T17:04:43.307302Z",
     "shell.execute_reply.started": "2022-06-06T17:04:35.832335Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import PipelineModel\n",
    "\n",
    "# Read the model from disk\n",
    "pipelineModel = PipelineModel.load(pipeline_model_path + \"/spark-logistic-regression-model\")\n",
    "\n",
    "# Apply machine learning pipeline to the data\n",
    "results = pipelineModel.transform(df)\n",
    "\n",
    "results.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2525f651-7675-4f47-a556-afd0208e0ae8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-06T17:04:46.513075Z",
     "iopub.status.busy": "2022-06-06T17:04:46.512538Z",
     "iopub.status.idle": "2022-06-06T17:04:46.607462Z",
     "shell.execute_reply": "2022-06-06T17:04:46.606690Z",
     "shell.execute_reply.started": "2022-06-06T17:04:46.513052Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "results = results.withColumn(\"processed_at\", current_timestamp())\n",
    "\n",
    "results = (\n",
    "    results.withColumn(\"probability\", results[\"probability\"].cast(\"String\"))\n",
    "    .withColumn(\n",
    "        \"probabilityre\",\n",
    "        split(regexp_replace(\"probability\", \"^\\[|\\]\", \"\"), \",\")[1].cast(DoubleType()),\n",
    "    )\n",
    "    .select(\"id\", \"probabilityre\", \"processed_at\")\n",
    "    .withColumnRenamed(\"probabilityre\", \"probability\")\n",
    ")\n",
    "\n",
    "results_kafka = results.select(\n",
    "    to_json(struct(\"id\", \"probability\", \"processed_at\")).alias(\"value\")\n",
    ")\n",
    "\n",
    "results_postgres = results.select(\n",
    "    \"id\", \"probability\", \"processed_at\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfed30ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_kafka.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3592aea8-f06c-41be-8de0-4033cf930de1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-06T17:04:46.613942Z",
     "iopub.status.busy": "2022-06-06T17:04:46.613243Z",
     "iopub.status.idle": "2022-06-06T17:04:46.617782Z",
     "shell.execute_reply": "2022-06-06T17:04:46.617108Z",
     "shell.execute_reply.started": "2022-06-06T17:04:46.613917Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "results_postgres.printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1bb0d0b",
   "metadata": {},
   "source": [
    "### Logging the data stream in the console"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5332f99b-a345-4383-94f5-885391a53c50",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-06T17:04:46.621809Z",
     "iopub.status.busy": "2022-06-06T17:04:46.621243Z",
     "iopub.status.idle": "2022-06-06T17:04:46.625335Z",
     "shell.execute_reply": "2022-06-06T17:04:46.624590Z",
     "shell.execute_reply.started": "2022-06-06T17:04:46.621784Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = results.select(\"id\", \"probability\", \"processed_at\") \\\n",
    "    .writeStream \\\n",
    "    .format(\"console\") \\\n",
    "    .start()\n",
    "\n",
    "time.sleep(10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "443db869",
   "metadata": {},
   "outputs": [],
   "source": [
    "query.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb48ce3",
   "metadata": {},
   "source": [
    "### Inserting data stream transformation results into another Apache Kafka topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e553b9fb-9c09-4340-8451-b0e95f653bc2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-06T17:04:46.791477Z",
     "iopub.status.busy": "2022-06-06T17:04:46.790890Z",
     "iopub.status.idle": "2022-06-06T17:04:46.795093Z",
     "shell.execute_reply": "2022-06-06T17:04:46.794281Z",
     "shell.execute_reply.started": "2022-06-06T17:04:46.791450Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = results_kafka.writeStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", KAFKA_HOST) \\\n",
    "    .option(\"topic\", \"predictions\") \\\n",
    "    .option(\"checkpointLocation\", \"checkpointLocation\") \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b66a34c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200c09c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec68d865",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63613986",
   "metadata": {},
   "outputs": [],
   "source": [
    "!kafka-console-consumer --bootstrap-server $(minikube service kafka-cluster-kafka-external-bootstrap --url -n demo) --topic predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f48bfa41",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "331d73dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "query.status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d75d429",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(query.recentProgress)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35208efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(query.recentProgress)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "775c5db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "query.lastProgress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a83bc97",
   "metadata": {},
   "outputs": [],
   "source": [
    "statistics = {}\n",
    "\n",
    "while True:\n",
    "\n",
    "  last = query.lastProgress\n",
    "  \n",
    "  if last[\"batchId\"] not in statistics.keys():\n",
    "    statistics[last[\"batchId\"]] = last"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46fc3221",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save statistics to json file\n",
    "with open('statistics-40replicas-0second.json', 'w') as f:\n",
    "    json.dump(statistics, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c58637",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff3dbac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "for i in range(100):\n",
    "\n",
    "  data = query.lastProgress\n",
    "  \n",
    "  print(\"inputRowsPerSecond: \", data[\"inputRowsPerSecond\"])\n",
    "  print(\"processedRowsPerSecond:\", data[\"processedRowsPerSecond\"])\n",
    "  print(\"\\n\")\n",
    "\n",
    "  time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae91c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "query.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0548163-9ba2-4b3d-8e6d-3db8ef943a6c",
   "metadata": {},
   "source": [
    "### Inserting data stream into PostgreSQL database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9947c7ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a PostgreSQL database with Docker\n",
    "!docker run -d -e POSTGRES_PASSWORD=postgres -p 5432:5432 --name postgres postgres:11.7-alpine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88640a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get PostgreSQL logs\n",
    "!docker logs postgres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc858ec",
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "%%sql postgresql://postgres:postgres@localhost:5432/postgres\n",
    "\n",
    "CREATE TABLE predictions (\n",
    "\tid DECIMAL(38, 0),\n",
    "\tprobability DOUBLE PRECISION,\n",
    "\tprocessed_at TIMESTAMP\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d2a08c6-d78e-4f90-9954-7d83b6ac36bb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-06T17:04:46.631436Z",
     "iopub.status.busy": "2022-06-06T17:04:46.630914Z",
     "iopub.status.idle": "2022-06-06T17:04:46.789470Z",
     "shell.execute_reply": "2022-06-06T17:04:46.788754Z",
     "shell.execute_reply.started": "2022-06-06T17:04:46.631413Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def foreach_batch_function(df, epoch_id):\n",
    "\n",
    "    df.write.format(\"jdbc\").option(\n",
    "        \"url\", \"jdbc:postgresql://localhost:5432/postgres\"\n",
    "    ).option(\"driver\", \"org.postgresql.Driver\").option(\"dbtable\", \"predictions\").option(\n",
    "        \"user\", \"postgres\"\n",
    "    ).option(\n",
    "        \"password\", \"postgres\"\n",
    "    ).mode(\n",
    "        \"append\"\n",
    "    ).save()\n",
    "\n",
    "query = results_postgres \\\n",
    "    .writeStream \\\n",
    "    .foreachBatch(foreach_batch_function) \\\n",
    "    .option(\"checkpointLocation\", \"checkpointLocation\") \\\n",
    "    .start()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea67771b",
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "%%sql postgresql://postgres:postgres@localhost:5432/postgres\n",
    "\n",
    "SELECT COUNT(*) FROM PREDICTIONS;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ccbc9c4",
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "%%sql postgresql://postgres:postgres@localhost:5432/postgres\n",
    "\n",
    "SELECT\n",
    "\t*\n",
    "FROM \n",
    "\tPREDICTIONS\n",
    "ORDER BY\n",
    "\tPROCESSED_AT DESC\n",
    "LIMIT 10;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaebcb0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop data streams\n",
    "query.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('spark-ml-training-xDY9FCqV-py3.10')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "69a199846c330755bc2557dd28c4dc3412422f74f1c83d05f31b126cc52659c8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
